{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Classification Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda numpy==1.14.0 --yes\n",
    "#!conda install -c conda-forge keras --yes\n",
    "#!conda install -c anaconda tensorflow-gpu --yes\n",
    "#!conda install -c anaconda gensim --yes\n",
    "#!pip install kapre\n",
    "#!pip install tensorboard\n",
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import sys\n",
    "import skvideo.io\n",
    "from skvideo.measure import scenedet\n",
    "import json\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy import sparse\n",
    "import urllib\n",
    "from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time as t\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "sys.path.append('lmtd')\n",
    "from lmtd9 import LMTD\n",
    "from lmtd9 import database as db\n",
    "from lmtd9 import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa for audio\n",
    "import librosa\n",
    "# And the display module for visualization\n",
    "import librosa.display\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "from sklearn import preprocessing\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffmpeg_load_audio import ffmpeg_load_audio\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "import urlparse, os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import operator\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU, Reshape\n",
    "from keras import applications\n",
    "from keras.layers import GlobalAveragePooling2D, merge, Input\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, BatchNormalization\n",
    "from keras.layers import Input, Convolution1D, GlobalMaxPooling1D, merge, Dense, Dropout\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n_cpus:', 8)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "gaussian_noise_lstm = 0.01\n",
    "lstm_size = 128\n",
    "gru_size = 64\n",
    "recurrent_dropout = 0.2\n",
    "dropout_cnn = 0.5\n",
    "gaussian_noise_cnn = 0.1\n",
    "model_output_path = \"/home/paulo/mestrado/MovieGenreClassifier.model\"\n",
    "validation_split = 0.2\n",
    "test_split = 0.2\n",
    "regularizer_lambda = 0.01\n",
    "GLOVE_DIR = \"/home/paulo/mestrado/glove.6B/\"\n",
    "min_word_frequency_word2vec = 5,\n",
    "embed_size_word2vec = 100\n",
    "context_window_word2vec = 10\n",
    "MAX_NB_WORDS = 50000\n",
    "max_sentence_len = 100\n",
    "maxlen = max_sentence_len\n",
    "min_sentence_length = 15\n",
    "sr = 44100 #sampling rate 44.1KHz\n",
    "audio_channels = 1 #mono\n",
    "width = 224 #224 for VGG16,VGG19,ResNet50 (299 for inceptionV3)\n",
    "height = 224  #224 for VGG16,VGG19,ResNet50 (299 for inceptionV3)  \n",
    "depth = 3 #rgb\n",
    "audio_channels = 2 #stereo\n",
    "audio_input_shape = (audio_channels, sr) #2-channels - 44.1KHz\n",
    "n_mels=64 #Mel Spectogram Size\n",
    "duration=60 #Uniform time in seconds for trailer music (60=1 minute)\n",
    "time_steps = 240\n",
    "low_level_features = 4 #low level features\n",
    "nb_features = 2048 #ltmd features\n",
    "nb_classes = 9\n",
    "conv_filters = 384\n",
    "dropout = 0.5\n",
    "max_epochs = 50\n",
    "n_mels=128\n",
    "n_hops=256\n",
    "n_mfcc=40\n",
    "nb_frames=1000\n",
    "num_classes=9\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()    \n",
    "print('n_cpus:',n_cpus)      \n",
    "number_gpus = 1\n",
    "number_cpus = n_cpus-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMTD_PATH = '/home/paulo/mestrado/lmtd'    \n",
    "lmtd = LMTD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(all_data):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(all_data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_glove_embeddings():\n",
    "    embeddings_index = {}\n",
    "    glove_file = 'glove.6B.{}d.txt'.format(embed_size_word2vec) #need to make both to match (embed_size_word2vec and globe index)\n",
    "    f = open(os.path.join(GLOVE_DIR, glove_file))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embeddings_matrices(embeddings_index, word_index):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, embed_size_word2vec))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embedding_layer(word_index, embedding_matrix):\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                embed_size_word2vec,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sentence_len,\n",
    "                                trainable=True)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftAttention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe(x):\n",
    "    return K.clip(x, K.common._EPSILON, 1.0 - K.common._EPSILON)\n",
    "\n",
    "class Wrapper(Layer):\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        self.layer = layer\n",
    "        super(Wrapper, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):       \n",
    "        super(Wrapper, self).build(input_shape)  # Be sure to call this somewhere!   \n",
    "       \n",
    "    def call(self, x):\n",
    "        super(Wrapper, self).call(x)\n",
    "\n",
    "class ProbabilityTensor(Wrapper):\n",
    "    \"\"\" function for turning 3d tensor to 2d probability matrix, which is the set of a_i's \"\"\"\n",
    "    def __init__(self, dense_function=None, *args, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(ndim=3)]\n",
    "        #layer = TimeDistributed(dense_function) or TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        layer = TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        super(ProbabilityTensor, self).__init__(layer, *args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(shape=input_shape)]\n",
    "        if K._BACKEND == 'tensorflow':\n",
    "            if not input_shape[1]:\n",
    "                raise Exception('When using TensorFlow, you should define '\n",
    "                                'explicitly the number of timesteps of '\n",
    "                                'your sequences.\\n'\n",
    "                                'If your first layer is an Embedding, '\n",
    "                                'make sure to pass it an \"input_length\" '\n",
    "                                'argument. Otherwise, make sure '\n",
    "                                'the first layer has '\n",
    "                                'an \"input_shape\" or \"batch_input_shape\" '\n",
    "                                'argument, including the time axis.')\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ProbabilityTensor, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # b,n,f -> b,n\n",
    "        #       s.t. \\sum_n n = 1\n",
    "        if isinstance(input_shape, (list,tuple)) and not isinstance(input_shape[0], int):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def squash_mask(self, mask):\n",
    "        if K.ndim(mask) == 2:\n",
    "            return mask\n",
    "        elif K.ndim(mask) == 3:\n",
    "            return K.any(mask, axis=-1)\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return self.squash_mask(mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        energy = K.squeeze(self.layer(x), 2)\n",
    "        p_matrix = K.softmax(energy)\n",
    "        if mask is not None:\n",
    "            mask = self.squash_mask(mask)\n",
    "            p_matrix = make_safe(p_matrix * mask)\n",
    "            p_matrix = (p_matrix / K.sum(p_matrix, axis=-1, keepdims=True))*mask\n",
    "        return p_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(ProbabilityTensor, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SoftAttentionConcat(ProbabilityTensor):\n",
    "    '''This will create the context vector and then concatenate it with the last output of the LSTM'''\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # b,n,f -> b,f where f is weighted features summed across n\n",
    "        return (input_shape[0], 2*input_shape[2])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None or mask.ndim==2:\n",
    "            return None\n",
    "        else:\n",
    "            raise Exception(\"Unexpected situation\")\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # b,n,f -> b,f via b,n broadcasted\n",
    "        p_vectors = K.expand_dims(super(SoftAttentionConcat, self).call(x, mask), 2)\n",
    "        expanded_p = K.repeat_elements(p_vectors, K.int_shape(x)[2], axis=2)\n",
    "        context = K.sum(expanded_p * x, axis=1)\n",
    "        last_out = x[:, -1, :]\n",
    "        return K.concatenate([context, last_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = np.load('plots.npz')\n",
    "plots.files\n",
    "all_data=plots['all_data'].tolist() \n",
    "del plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = setup_tokenizer(all_data)\n",
    "word_index = tokenizer.word_index\n",
    "embeddings_index = setup_glove_embeddings()\n",
    "embedding_matrix = setup_embeddings_matrices(embeddings_index, word_index)\n",
    "embedding_layer = setup_embedding_layer(word_index, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    for k, v in result.iteritems():\n",
    "        try:\n",
    "            print '{:<15s}'.format(lmtd.genres[k][2:]),\n",
    "        except IndexError:\n",
    "            print '{:<15s}'.format(str(k)),\n",
    "        print '{:5.4f}'.format(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with unbalaced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "y_train=trainfiles['y_train']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "y_val=valfiles['y_val']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "y_test=testfiles['y_test']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('class_weight:', {0: 0.1111864406779661, 1: 0.07322033898305084, 2: 0.2047457627118644, 3: 0.08203389830508474, 4: 0.2671186440677966, 5: 0.05288135593220339, 6: 0.08271186440677966, 7: 0.03864406779661017, 8: 0.08745762711864406})\n"
     ]
    }
   ],
   "source": [
    "class_weight = {}\n",
    "weights = np.sum(y_test,axis=0)/np.sum(y_test)\n",
    "for i in range(nb_classes):\n",
    "    class_weight[i] = weights[i]\n",
    "print('class_weight:',class_weight)\n",
    "class_weight = class_weight.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles_text = np.load('train_matrices_text.npz', mmap_mode='r')\n",
    "X_train_text=trainfiles_text['X_train_text'] \n",
    "del trainfiles_text\n",
    "\n",
    "valfiles_text = np.load('val_matrices_text.npz', mmap_mode='r')\n",
    "X_val_text=valfiles_text['X_val_text']\n",
    "del valfiles_text\n",
    "\n",
    "testfiles_text = np.load('test_matrices_text.npz', mmap_mode='r')\n",
    "X_test_text=testfiles_text['X_test_text']\n",
    "del testfiles_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 21, 3, 10, 831187))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "full_plot_text (InputLayer)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2018900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 256)          234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 256)          394240    \n",
      "_________________________________________________________________\n",
      "soft_attention_concat_1 (Sof (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "new_features_text (Dense)    (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 36873     \n",
      "=================================================================\n",
      "Total params: 13,703,773\n",
      "Trainable params: 13,702,749\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/paulo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 65s 23ms/step - loss: 0.4935 - acc: 0.7821 - val_loss: 0.4622 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46219, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.4435 - acc: 0.8016 - val_loss: 0.4090 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46219 to 0.40896, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.4067 - acc: 0.8147 - val_loss: 0.4276 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3913 - acc: 0.8198 - val_loss: 0.4259 - val_acc: 0.8051\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 61s 21ms/step - loss: 0.3738 - acc: 0.8281 - val_loss: 0.3849 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40896 to 0.38489, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3620 - acc: 0.8336 - val_loss: 0.3784 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.38489 to 0.37840, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3530 - acc: 0.8383 - val_loss: 0.3731 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37840 to 0.37309, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3398 - acc: 0.8462 - val_loss: 0.3666 - val_acc: 0.8304\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37309 to 0.36660, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3369 - acc: 0.8473 - val_loss: 0.3849 - val_acc: 0.8247\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3270 - acc: 0.8544 - val_loss: 0.3627 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.36660 to 0.36267, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3135 - acc: 0.8579 - val_loss: 0.3620 - val_acc: 0.8369\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36267 to 0.36201, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3064 - acc: 0.8628 - val_loss: 0.3638 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2955 - acc: 0.8692 - val_loss: 0.3610 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36201 to 0.36097, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2904 - acc: 0.8701 - val_loss: 0.3564 - val_acc: 0.8390\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.36097 to 0.35640, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 15/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2813 - acc: 0.8754 - val_loss: 0.3620 - val_acc: 0.8408\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2726 - acc: 0.8789 - val_loss: 0.3709 - val_acc: 0.8378\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2550 - acc: 0.8873 - val_loss: 0.3623 - val_acc: 0.8408\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2509 - acc: 0.8897 - val_loss: 0.3645 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2323 - acc: 0.8992 - val_loss: 0.3701 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "773/773 [==============================] - 4s 5ms/step\n",
      "[0.36618445684499606, 0.8335489595229382]\n",
      "Action          0.6981\n",
      "Adventure       0.7105\n",
      "Comedy          0.7871\n",
      "Crime           0.5801\n",
      "Drama           0.7819\n",
      "Horror          0.6376\n",
      "Romance         0.4735\n",
      "SciFi           0.5690\n",
      "Thriller        0.4337\n",
      "macro           0.6302\n",
      "micro           0.6796\n",
      "weighted        0.6800\n",
      "('End:', datetime.datetime(2018, 6, 4, 21, 3, 10, 831187))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    # Branch 1 - RNN for text (Full Plot)\n",
    "    sequence_input = Input(shape=(max_sentence_len,), dtype='int32', name='full_plot_text')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    lstm = Bidirectional(LSTM(lstm_size, return_sequences=True, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n",
    "    lstm2 = Bidirectional(LSTM(lstm_size, return_sequences=True, recurrent_dropout=recurrent_dropout))(lstm)\n",
    "    attention_1 = SoftAttentionConcat()(lstm2)\n",
    "    batch1 = BatchNormalization()(attention_1)\n",
    "    branch_1 = Dense(1024, activation='relu')(batch1)\n",
    "    branch_1 = Dropout(0.5)(branch_1)\n",
    "        \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = branch_1\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_text')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    text_model = Model(inputs=[sequence_input], outputs=[predictions]) \n",
    "    \n",
    "    text_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_text.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    text_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = text_model.fit([X_train_text], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([X_val_text], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    text_model.save('MovieGenreClassifier_text.h5')\n",
    "    \n",
    "    y_pred_test = text_model.predict([X_test_text]) #\n",
    "    print(text_model.evaluate([X_test_text], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_text'\n",
    "    text_intermediate_layer_model = Model(inputs=[sequence_input],\n",
    "                                 outputs=text_model.get_layer(layer_name).output)\n",
    "    text_intermediate_output_train = text_intermediate_layer_model.predict([X_train_text])\n",
    "    text_intermediate_output_val = text_intermediate_layer_model.predict([X_val_text])\n",
    "    text_intermediate_output_test = text_intermediate_layer_model.predict([X_test_text])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release memory\n",
    "del X_train_text\n",
    "del X_val_text\n",
    "del X_test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poster NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "train_poster_images=trainfiles['train_poster_images']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "val_poster_images=valfiles['val_poster_images']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "test_poster_images=testfiles['test_poster_images']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 21, 23, 33, 347305))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "poster_image (InputLayer)    (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 222, 222, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 222, 222, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "new_features_poster (Dense)  (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 36873     \n",
      "=================================================================\n",
      "Total params: 10,572,853\n",
      "Trainable params: 10,572,783\n",
      "Non-trainable params: 70\n",
      "_________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 27s 9ms/step - loss: 0.4927 - acc: 0.7839 - val_loss: 0.4594 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45938, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4648 - acc: 0.7968 - val_loss: 0.4558 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45938 to 0.45578, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4604 - acc: 0.7988 - val_loss: 0.4491 - val_acc: 0.8158\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45578 to 0.44909, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4584 - acc: 0.8002 - val_loss: 0.4434 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44909 to 0.44344, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4557 - acc: 0.8018 - val_loss: 0.4412 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44344 to 0.44122, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4541 - acc: 0.8000 - val_loss: 0.4349 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44122 to 0.43494, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4537 - acc: 0.8012 - val_loss: 0.4504 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4505 - acc: 0.8028 - val_loss: 0.4387 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4523 - acc: 0.8032 - val_loss: 0.4368 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4490 - acc: 0.8010 - val_loss: 0.4277 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.43494 to 0.42774, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4481 - acc: 0.8031 - val_loss: 0.4340 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4461 - acc: 0.8027 - val_loss: 0.4248 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.42774 to 0.42477, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4470 - acc: 0.8035 - val_loss: 0.4318 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4449 - acc: 0.8035 - val_loss: 0.4306 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4444 - acc: 0.8039 - val_loss: 0.4197 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.42477 to 0.41965, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4428 - acc: 0.8070 - val_loss: 0.4224 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4424 - acc: 0.8076 - val_loss: 0.4247 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4423 - acc: 0.8048 - val_loss: 0.4283 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4399 - acc: 0.8084 - val_loss: 0.4225 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4380 - acc: 0.8086 - val_loss: 0.4225 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "773/773 [==============================] - 2s 3ms/step\n",
      "[0.43041565154533334, 0.8137128286034756]\n",
      "Action          0.2914\n",
      "Adventure       0.2798\n",
      "Comedy          0.7258\n",
      "Crime           0.2470\n",
      "Drama           0.6119\n",
      "Horror          0.2558\n",
      "Romance         0.3693\n",
      "SciFi           0.1818\n",
      "Thriller        0.3325\n",
      "macro           0.3661\n",
      "micro           0.5194\n",
      "weighted        0.4654\n",
      "('End:', datetime.datetime(2018, 6, 4, 21, 23, 33, 347305))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "                \n",
    "    # Branch 2 - CNN for images (Poster)\n",
    "    '''\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    # Freeze Inception's weights - we don't want to train these\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # add a fully connected layer after Inception - we do want to train these\n",
    "    branch2 = base_model.output               \n",
    "    '''\n",
    "    poster_input = Input(shape=(width, height, depth), name='poster_image')   \n",
    "    branch2 = BatchNormalization()(poster_input)\n",
    "    branch2 = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    branch2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    branch2 = BatchNormalization()(branch2)\n",
    "    branch2 = MaxPooling2D(pool_size=(2, 2))(branch2)    \n",
    "    branch2 = Dropout(0.25)(branch2)               \n",
    "    branch2 = GlobalAveragePooling2D()(branch2)\n",
    "    branch2 = Dense(1024, activation='relu')(branch2)\n",
    "    branch2 = Dropout(0.5)(branch2)  \n",
    "    \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = branch2\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_poster')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    poster_model = Model(inputs=[poster_input], outputs=[predictions]) \n",
    "    \n",
    "    poster_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_poster.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    poster_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = poster_model.fit([train_poster_images], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([val_poster_images], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    poster_model.save('MovieGenreClassifier_poster.h5')\n",
    "    \n",
    "    y_pred_test = poster_model.predict([test_poster_images]) #\n",
    "    print(poster_model.evaluate([test_poster_images], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_poster'\n",
    "    poster_intermediate_layer_model = Model(inputs=[poster_input],\n",
    "                                 outputs=poster_model.get_layer(layer_name).output)\n",
    "    poster_intermediate_output_train = poster_intermediate_layer_model.predict([train_poster_images])\n",
    "    poster_intermediate_output_val = poster_intermediate_layer_model.predict([val_poster_images])\n",
    "    poster_intermediate_output_test = poster_intermediate_layer_model.predict([test_poster_images])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release memory\n",
    "del train_poster_images\n",
    "del val_poster_images\n",
    "del test_poster_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio NN / Low Level NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "train_spectograms=trainfiles['train_spectograms']\n",
    "train_X=trainfiles['train_X']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "val_spectograms=valfiles['val_spectograms']\n",
    "val_X=valfiles['val_X']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "testfiles.files\n",
    "test_spectograms=testfiles['test_spectograms']\n",
    "test_X=testfiles['test_X'] \n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 21, 31, 33, 610332))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/librosa/filters.py:271: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n",
      "/home/paulo/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:45: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/paulo/anaconda2/lib/python2.7/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "audio_spectrogram (InputLayer)  (None, 2, 44100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trainable_stft (Melspectrogram) (None, 64, 345, 2)   74304       audio_spectrogram[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "additive_noise_1 (AdditiveNoise (None, 64, 345, 2)   0           trainable_stft[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normalization2d_1 (Normalizatio (None, 64, 345, 2)   0           additive_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 345, 32)  608         normalization2d_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 62, 343, 32)  9248        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 62, 343, 32)  128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 31, 171, 32)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 31, 171, 32)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 32)           0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         33792       global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "low_level_features (InputLayer) (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 4)            16          low_level_features[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 1028)         0           dropout_9[0][0]                  \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2048)         2107392     merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2048)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "new_features_audio (Dense)      (None, 4096)         8392704     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 4096)         0           new_features_audio[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 9)            36873       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,655,065\n",
      "Trainable params: 10,580,689\n",
      "Non-trainable params: 74,376\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 13s 4ms/step - loss: 0.4909 - acc: 0.7814 - val_loss: 0.5372 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53716, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4783 - acc: 0.7891 - val_loss: 0.5275 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53716 to 0.52753, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4757 - acc: 0.7900 - val_loss: 0.5205 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52753 to 0.52046, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4754 - acc: 0.7901 - val_loss: 0.5095 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52046 to 0.50954, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4754 - acc: 0.7900 - val_loss: 0.5038 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50954 to 0.50382, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4728 - acc: 0.7913 - val_loss: 0.4999 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50382 to 0.49992, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4733 - acc: 0.7899 - val_loss: 0.4968 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49992 to 0.49678, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4730 - acc: 0.7895 - val_loss: 0.4899 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49678 to 0.48994, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4731 - acc: 0.7912 - val_loss: 0.4883 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48994 to 0.48833, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4711 - acc: 0.7927 - val_loss: 0.4878 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48833 to 0.48784, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4721 - acc: 0.7927 - val_loss: 0.4859 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48784 to 0.48586, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4709 - acc: 0.7929 - val_loss: 0.4793 - val_acc: 0.7929\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48586 to 0.47925, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4701 - acc: 0.7928 - val_loss: 0.4800 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4714 - acc: 0.7913 - val_loss: 0.4756 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.47925 to 0.47564, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4711 - acc: 0.7924 - val_loss: 0.4749 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47564 to 0.47494, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4701 - acc: 0.7926 - val_loss: 0.4742 - val_acc: 0.7929\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.47494 to 0.47419, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4687 - acc: 0.7939 - val_loss: 0.4741 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.47419 to 0.47409, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 18/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4695 - acc: 0.7929 - val_loss: 0.4735 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47409 to 0.47351, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 19/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4697 - acc: 0.7932 - val_loss: 0.4732 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47351 to 0.47317, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 20/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4677 - acc: 0.7934 - val_loss: 0.4669 - val_acc: 0.7977\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.47317 to 0.46691, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 21/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4695 - acc: 0.7934 - val_loss: 0.4674 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4683 - acc: 0.7916 - val_loss: 0.4669 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.46691 to 0.46686, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 23/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4690 - acc: 0.7921 - val_loss: 0.4635 - val_acc: 0.7965\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.46686 to 0.46352, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 24/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4669 - acc: 0.7958 - val_loss: 0.4693 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4680 - acc: 0.7920 - val_loss: 0.4665 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4664 - acc: 0.7956 - val_loss: 0.4682 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4671 - acc: 0.7950 - val_loss: 0.4711 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4646 - acc: 0.7958 - val_loss: 0.4674 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "773/773 [==============================] - 1s 2ms/step\n",
      "[0.5040972181156104, 0.7879833397088045]\n",
      "Action          0.2122\n",
      "Adventure       0.1397\n",
      "Comedy          0.3907\n",
      "Crime           0.1565\n",
      "Drama           0.5097\n",
      "Horror          0.1009\n",
      "Romance         0.1578\n",
      "SciFi           0.0737\n",
      "Thriller        0.1669\n",
      "macro           0.2120\n",
      "micro           0.2721\n",
      "weighted        0.2986\n",
      "('End:', datetime.datetime(2018, 6, 4, 21, 31, 33, 610332))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    # Branch 3 - CNN for spectrogram image (Audio)\n",
    "    audio_inputs = Input(shape=audio_input_shape, name='audio_spectrogram')\n",
    "    #A mel-spectrogram layer    \n",
    "    branch3 = Melspectrogram(n_dft=256, n_hop=128,\n",
    "                             padding='same', sr=sr, n_mels=64,\n",
    "                             fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                             return_decibel_melgram=False, trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='trainable_stft')(audio_inputs)\n",
    "    # Maybe some additive white noise.\n",
    "    branch3 = AdditiveNoise(power=0.2)(branch3)\n",
    "    # If you wanna normalise it per-frequency\n",
    "    branch3 = Normalization2D(str_axis='freq')(branch3) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "    #CNN\n",
    "    branch3 = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    branch3 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    branch3 = BatchNormalization()(branch3)\n",
    "    branch3 = MaxPooling2D(pool_size=(2, 2))(branch3)\n",
    "    branch3 = Dropout(0.25)(branch3)   \n",
    "    branch3 = GlobalAveragePooling2D()(branch3)\n",
    "    branch3 = Dense(1024, activation='relu')(branch3)\n",
    "    branch3 = Dropout(0.5)(branch3)\n",
    "        \n",
    "    #Branch5 - Low Level Features\n",
    "    low_level_inputs = Input(shape=(low_level_features,), name='low_level_features')\n",
    "    branch5 = BatchNormalization()(low_level_inputs)\n",
    "    \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([branch3, branch5], mode='concat')\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_audio')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    audio_model = Model(inputs=[audio_inputs, low_level_inputs], outputs=[predictions]) \n",
    "    \n",
    "    audio_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_audio.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    audio_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = audio_model.fit([train_spectograms,train_X], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([val_spectograms,val_X ], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    audio_model.save('MovieGenreClassifier_audio.h5')\n",
    "    \n",
    "    y_pred_test = audio_model.predict([test_spectograms, test_X]) #\n",
    "    print(audio_model.evaluate([test_spectograms, test_X], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_audio'\n",
    "    audio_intermediate_layer_model = Model(inputs=[audio_inputs, low_level_inputs],\n",
    "                                 outputs=audio_model.get_layer(layer_name).output)\n",
    "    audio_intermediate_output_train = audio_intermediate_layer_model.predict([train_spectograms, train_X])\n",
    "    audio_intermediate_output_val = audio_intermediate_layer_model.predict([val_spectograms, val_X])\n",
    "    audio_intermediate_output_test = audio_intermediate_layer_model.predict([test_spectograms, test_X])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_spectograms\n",
    "del train_X\n",
    "\n",
    "del val_spectograms\n",
    "del val_X\n",
    "\n",
    "del test_spectograms\n",
    "del test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "train_lmtd_features=trainfiles['train_lmtd_features']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "val_lmtd_features=valfiles['val_lmtd_features']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "test_lmtd_features=testfiles['test_lmtd_features']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 21, 56, 11, 40867))\n",
      "WARNING:tensorflow:From /home/paulo/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lmtd9_features (InputLayer)  (None, 240, 2048)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 240, 2048)         8192      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 238, 384)          2359680   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 238, 384)          1536      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 238, 384)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2048)              788480    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "new_features_video (Dense)   (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 36873     \n",
      "=================================================================\n",
      "Total params: 15,783,817\n",
      "Trainable params: 15,778,953\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 152s 53ms/step - loss: 0.5955 - acc: 0.7512 - val_loss: 0.5270 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52702, saving model to tensorflow/tmp/weigths_mir_classifier_video.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 215s 75ms/step - loss: 0.5066 - acc: 0.7782 - val_loss: 0.5523 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 84s 29ms/step - loss: 0.4992 - acc: 0.7789 - val_loss: 0.5544 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 20s 7ms/step - loss: 0.4885 - acc: 0.7840 - val_loss: 0.5586 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 20s 7ms/step - loss: 0.4773 - acc: 0.7862 - val_loss: 0.5327 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 20s 7ms/step - loss: 0.4721 - acc: 0.7897 - val_loss: 0.5329 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "773/773 [==============================] - 13s 17ms/step\n",
      "[0.539202792215779, 0.8099755821376076]\n",
      "Action          0.5738\n",
      "Adventure       0.4120\n",
      "Comedy          0.7879\n",
      "Crime           0.2839\n",
      "Drama           0.5824\n",
      "Horror          0.2402\n",
      "Romance         0.3399\n",
      "SciFi           0.2196\n",
      "Thriller        0.2893\n",
      "macro           0.4143\n",
      "micro           0.5491\n",
      "weighted        0.5087\n",
      "('End:', datetime.datetime(2018, 6, 4, 21, 56, 11, 40867))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "            \n",
    "    #Branch 4 - LTMD9 Features                    \n",
    "    lmtd_inputs = Input(shape=(time_steps, nb_features), name='lmtd9_features')    \n",
    "    branch4 = BatchNormalization()(lmtd_inputs) \n",
    "    branch4 = Convolution1D(conv_filters, kernel_size=3)(branch4)\n",
    "    branch4 = BatchNormalization()(branch4)\n",
    "    branch4 = Activation('relu')(branch4)\n",
    "    branch4 = GlobalMaxPooling1D()(branch4)\n",
    "    branch4 = Dropout(dropout)(branch4)\n",
    "    branch4 = Dense(2048, activation='relu')(branch4)\n",
    "    branch4 = Dropout(0.5)(branch4)\n",
    "       \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = branch4\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_video')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    video_model = Model(inputs=[lmtd_inputs], outputs=[predictions]) \n",
    "    \n",
    "    video_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate/10)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_video.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    video_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = video_model.fit([train_lmtd_features], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([val_lmtd_features], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    video_model.save('MovieGenreClassifier_video.h5')\n",
    "    \n",
    "    y_pred_test = video_model.predict([test_lmtd_features]) #\n",
    "    print(video_model.evaluate([test_lmtd_features], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_video'\n",
    "    video_intermediate_layer_model = Model(inputs=[lmtd_inputs],\n",
    "                                 outputs=video_model.get_layer(layer_name).output)\n",
    "    video_intermediate_output_train = video_intermediate_layer_model.predict([train_lmtd_features])\n",
    "    video_intermediate_output_val = video_intermediate_layer_model.predict([val_lmtd_features])\n",
    "    video_intermediate_output_test = video_intermediate_layer_model.predict([test_lmtd_features])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_lmtd_features\n",
    "\n",
    "del val_lmtd_features\n",
    "\n",
    "del test_lmtd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "text_intermediate_output_train = np.reshape(text_intermediate_output_train,(text_intermediate_output_train.shape[0],text_intermediate_output_train.shape[1]))\n",
    "text_intermediate_output_val = np.reshape(text_intermediate_output_val,(poster_intermediate_output_val.shape[0],poster_intermediate_output_val.shape[1]))\n",
    "text_intermediate_output_test = np.reshape(text_intermediate_output_test,(poster_intermediate_output_test.shape[0],poster_intermediate_output_test.shape[1]))\n",
    "'''\n",
    "print(text_intermediate_output_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "poster_intermediate_output_train = np.reshape(poster_intermediate_output_train,(poster_intermediate_output_train.shape[0],poster_intermediate_output_train.shape[1]))\n",
    "poster_intermediate_output_val = np.reshape(poster_intermediate_output_val,(poster_intermediate_output_val.shape[0],poster_intermediate_output_val.shape[1]))\n",
    "poster_intermediate_output_test = np.reshape(poster_intermediate_output_test,(poster_intermediate_output_test.shape[0],poster_intermediate_output_test.shape[1]))\n",
    "'''\n",
    "print(poster_intermediate_output_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "audio_intermediate_output_train = np.reshape(audio_intermediate_output_train,(audio_intermediate_output_train.shape[0],audio_intermediate_output_train.shape[1]))\n",
    "audio_intermediate_output_val = np.reshape(audio_intermediate_output_val,(audio_intermediate_output_val.shape[0],audio_intermediate_output_val.shape[1]))\n",
    "audio_intermediate_output_test = np.reshape(audio_intermediate_output_test,(audio_intermediate_output_test.shape[0],audio_intermediate_output_test.shape[1]))\n",
    "'''\n",
    "print(audio_intermediate_output_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "video_intermediate_output_train = np.reshape(video_intermediate_output_train,(video_intermediate_output_train.shape[0], video_intermediate_output_train.shape[1]))\n",
    "video_intermediate_output_val = np.reshape(video_intermediate_output_val,(video_intermediate_output_val.shape[0], video_intermediate_output_val.shape[1]))\n",
    "video_intermediate_output_test = np.reshape(video_intermediate_output_test,(video_intermediate_output_test.shape[0], video_intermediate_output_test.shape[1]))\n",
    "'''\n",
    "print(video_intermediate_output_train.shape)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late fusion NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 22, 14, 43, 815466))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_features (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "poster_features (InputLayer)    (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "audio_features (InputLayer)     (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "video_features (InputLayer)     (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 16384)        0           text_features[0][0]              \n",
      "                                                                 poster_features[0][0]            \n",
      "                                                                 audio_features[0][0]             \n",
      "                                                                 video_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2048)         33556480    merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 2048)         0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "new_features (Dense)            (None, 4096)         8392704     dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 4096)         0           new_features[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 9)            36873       dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 41,986,057\n",
      "Trainable params: 41,986,057\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 38s 13ms/step - loss: 0.4576 - acc: 0.8072 - val_loss: 0.3607 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36071, saving model to tensorflow/tmp/weigths_mir_classifier_full.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 8s 3ms/step - loss: 0.3012 - acc: 0.8722 - val_loss: 0.3235 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36071 to 0.32348, saving model to tensorflow/tmp/weigths_mir_classifier_full.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 7s 3ms/step - loss: 0.2472 - acc: 0.8966 - val_loss: 0.3227 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32348 to 0.32266, saving model to tensorflow/tmp/weigths_mir_classifier_full.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 7s 2ms/step - loss: 0.2250 - acc: 0.9039 - val_loss: 0.3293 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 7s 2ms/step - loss: 0.2143 - acc: 0.9107 - val_loss: 0.3345 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 7s 2ms/step - loss: 0.2071 - acc: 0.9126 - val_loss: 0.3372 - val_acc: 0.8592\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 7s 2ms/step - loss: 0.2004 - acc: 0.9145 - val_loss: 0.3458 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 7s 2ms/step - loss: 0.1958 - acc: 0.9177 - val_loss: 0.3446 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "773/773 [==============================] - 0s 171us/step\n",
      "[0.35274442378793003, 0.8575535594723049]\n",
      "Action          0.7220\n",
      "Adventure       0.7250\n",
      "Comedy          0.8467\n",
      "Crime           0.5853\n",
      "Drama           0.7944\n",
      "Horror          0.6719\n",
      "Romance         0.5022\n",
      "SciFi           0.5668\n",
      "Thriller        0.4696\n",
      "macro           0.6538\n",
      "micro           0.7211\n",
      "weighted        0.7070\n",
      "('End:', datetime.datetime(2018, 6, 4, 22, 14, 43, 815466))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "            \n",
    "    text_inputs = Input(shape=(4096,), name='text_features')    \n",
    "    poster_inputs = Input(shape=(4096,), name='poster_features')    \n",
    "    audio_inputs = Input(shape=(4096,), name='audio_features')    \n",
    "    video_inputs = Input(shape=(4096,), name='video_features')    \n",
    "           \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([text_inputs, poster_inputs, audio_inputs, video_inputs], mode='concat')\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    full_model = Model(inputs=[text_inputs, poster_inputs,  audio_inputs, video_inputs], outputs=[predictions]) \n",
    "    \n",
    "    full_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate/10)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_full.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    full_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = full_model.fit([text_intermediate_output_train, poster_intermediate_output_train, audio_intermediate_output_train, video_intermediate_output_train], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([text_intermediate_output_val, poster_intermediate_output_val, audio_intermediate_output_val, video_intermediate_output_val], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    full_model.save('MovieGenreClassifier_full.h5')\n",
    "    \n",
    "    y_pred_test = full_model.predict([text_intermediate_output_test, poster_intermediate_output_test, audio_intermediate_output_test, video_intermediate_output_test]) #\n",
    "    print(full_model.evaluate([text_intermediate_output_test, poster_intermediate_output_test, audio_intermediate_output_test, video_intermediate_output_test], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)                                   \n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

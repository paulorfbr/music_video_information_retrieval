{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Classification Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda numpy==1.14.0 --yes\n",
    "#!conda install -c conda-forge keras --yes\n",
    "#!conda install -c anaconda tensorflow-gpu --yes\n",
    "#!conda install -c anaconda gensim --yes\n",
    "#!pip install kapre\n",
    "#!pip install tensorboard\n",
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import sys\n",
    "import skvideo.io\n",
    "from skvideo.measure import scenedet\n",
    "import json\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy import sparse\n",
    "import urllib\n",
    "from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time as t\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "sys.path.append('lmtd')\n",
    "from lmtd9 import LMTD\n",
    "from lmtd9 import database as db\n",
    "from lmtd9 import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa for audio\n",
    "import librosa\n",
    "# And the display module for visualization\n",
    "import librosa.display\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "from sklearn import preprocessing\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffmpeg_load_audio import ffmpeg_load_audio\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "import urlparse, os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import operator\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU, Reshape\n",
    "from keras import applications\n",
    "from keras.layers import GlobalAveragePooling2D, merge, Input\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, BatchNormalization\n",
    "from keras.layers import Input, Convolution1D, GlobalMaxPooling1D, merge, Dense, Dropout\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n_cpus:', 8)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "gaussian_noise_lstm = 0.01\n",
    "lstm_size = 128\n",
    "gru_size = 64\n",
    "recurrent_dropout = 0.2\n",
    "dropout_cnn = 0.5\n",
    "gaussian_noise_cnn = 0.1\n",
    "model_output_path = \"/home/paulo/mestrado/MovieGenreClassifier.model\"\n",
    "validation_split = 0.2\n",
    "test_split = 0.2\n",
    "regularizer_lambda = 0.01\n",
    "GLOVE_DIR = \"/home/paulo/mestrado/glove.6B/\"\n",
    "min_word_frequency_word2vec = 5,\n",
    "embed_size_word2vec = 100\n",
    "context_window_word2vec = 10\n",
    "MAX_NB_WORDS = 50000\n",
    "max_sentence_len = 100\n",
    "maxlen = max_sentence_len\n",
    "min_sentence_length = 15\n",
    "sr = 44100 #sampling rate 44.1KHz\n",
    "audio_channels = 1 #mono\n",
    "width = 224 #224 for VGG16,VGG19,ResNet50 (299 for inceptionV3)\n",
    "height = 224  #224 for VGG16,VGG19,ResNet50 (299 for inceptionV3)  \n",
    "depth = 3 #rgb\n",
    "audio_channels = 2 #stereo\n",
    "audio_input_shape = (audio_channels, sr) #2-channels - 44.1KHz\n",
    "n_mels=64 #Mel Spectogram Size\n",
    "duration=60 #Uniform time in seconds for trailer music (60=1 minute)\n",
    "time_steps = 240\n",
    "low_level_features = 4 #low level features\n",
    "nb_features = 2048 #ltmd features\n",
    "nb_classes = 9\n",
    "conv_filters = 384\n",
    "dropout = 0.5\n",
    "max_epochs = 50\n",
    "n_mels=128\n",
    "n_hops=256\n",
    "n_mfcc=40\n",
    "nb_frames=1000\n",
    "num_classes=9\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()    \n",
    "print('n_cpus:',n_cpus)      \n",
    "number_gpus = 1\n",
    "number_cpus = n_cpus-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMTD_PATH = '/home/paulo/mestrado/lmtd'    \n",
    "lmtd = LMTD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(all_data):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(all_data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_glove_embeddings():\n",
    "    embeddings_index = {}\n",
    "    glove_file = 'glove.6B.{}d.txt'.format(embed_size_word2vec) #need to make both to match (embed_size_word2vec and globe index)\n",
    "    f = open(os.path.join(GLOVE_DIR, glove_file))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embeddings_matrices(embeddings_index, word_index):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, embed_size_word2vec))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embedding_layer(word_index, embedding_matrix):\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                embed_size_word2vec,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sentence_len,\n",
    "                                trainable=True)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftAttention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe(x):\n",
    "    return K.clip(x, K.common._EPSILON, 1.0 - K.common._EPSILON)\n",
    "\n",
    "class Wrapper(Layer):\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        self.layer = layer\n",
    "        super(Wrapper, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):       \n",
    "        super(Wrapper, self).build(input_shape)  # Be sure to call this somewhere!   \n",
    "       \n",
    "    def call(self, x):\n",
    "        super(Wrapper, self).call(x)\n",
    "\n",
    "class ProbabilityTensor(Wrapper):\n",
    "    \"\"\" function for turning 3d tensor to 2d probability matrix, which is the set of a_i's \"\"\"\n",
    "    def __init__(self, dense_function=None, *args, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(ndim=3)]\n",
    "        #layer = TimeDistributed(dense_function) or TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        layer = TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        super(ProbabilityTensor, self).__init__(layer, *args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(shape=input_shape)]\n",
    "        if K._BACKEND == 'tensorflow':\n",
    "            if not input_shape[1]:\n",
    "                raise Exception('When using TensorFlow, you should define '\n",
    "                                'explicitly the number of timesteps of '\n",
    "                                'your sequences.\\n'\n",
    "                                'If your first layer is an Embedding, '\n",
    "                                'make sure to pass it an \"input_length\" '\n",
    "                                'argument. Otherwise, make sure '\n",
    "                                'the first layer has '\n",
    "                                'an \"input_shape\" or \"batch_input_shape\" '\n",
    "                                'argument, including the time axis.')\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ProbabilityTensor, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # b,n,f -> b,n\n",
    "        #       s.t. \\sum_n n = 1\n",
    "        if isinstance(input_shape, (list,tuple)) and not isinstance(input_shape[0], int):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def squash_mask(self, mask):\n",
    "        if K.ndim(mask) == 2:\n",
    "            return mask\n",
    "        elif K.ndim(mask) == 3:\n",
    "            return K.any(mask, axis=-1)\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return self.squash_mask(mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        energy = K.squeeze(self.layer(x), 2)\n",
    "        p_matrix = K.softmax(energy)\n",
    "        if mask is not None:\n",
    "            mask = self.squash_mask(mask)\n",
    "            p_matrix = make_safe(p_matrix * mask)\n",
    "            p_matrix = (p_matrix / K.sum(p_matrix, axis=-1, keepdims=True))*mask\n",
    "        return p_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(ProbabilityTensor, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SoftAttentionConcat(ProbabilityTensor):\n",
    "    '''This will create the context vector and then concatenate it with the last output of the LSTM'''\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # b,n,f -> b,f where f is weighted features summed across n\n",
    "        return (input_shape[0], 2*input_shape[2])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None or mask.ndim==2:\n",
    "            return None\n",
    "        else:\n",
    "            raise Exception(\"Unexpected situation\")\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # b,n,f -> b,f via b,n broadcasted\n",
    "        p_vectors = K.expand_dims(super(SoftAttentionConcat, self).call(x, mask), 2)\n",
    "        expanded_p = K.repeat_elements(p_vectors, K.int_shape(x)[2], axis=2)\n",
    "        context = K.sum(expanded_p * x, axis=1)\n",
    "        last_out = x[:, -1, :]\n",
    "        return K.concatenate([context, last_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = np.load('plots.npz')\n",
    "plots.files\n",
    "all_data=plots['all_data'].tolist() \n",
    "del plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = setup_tokenizer(all_data)\n",
    "word_index = tokenizer.word_index\n",
    "embeddings_index = setup_glove_embeddings()\n",
    "embedding_matrix = setup_embeddings_matrices(embeddings_index, word_index)\n",
    "embedding_layer = setup_embedding_layer(word_index, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    for k, v in result.iteritems():\n",
    "        try:\n",
    "            print '{:<15s}'.format(lmtd.genres[k][2:]),\n",
    "        except IndexError:\n",
    "            print '{:<15s}'.format(str(k)),\n",
    "        print '{:5.4f}'.format(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with unbalaced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "y_train=trainfiles['y_train']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "y_val=valfiles['y_val']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "y_test=testfiles['y_test']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('class_weight:', {0: 0.1111864406779661, 1: 0.07322033898305084, 2: 0.2047457627118644, 3: 0.08203389830508474, 4: 0.2671186440677966, 5: 0.05288135593220339, 6: 0.08271186440677966, 7: 0.03864406779661017, 8: 0.08745762711864406})\n"
     ]
    }
   ],
   "source": [
    "class_weight = {}\n",
    "weights = np.sum(y_test,axis=0)/np.sum(y_test)\n",
    "for i in range(nb_classes):\n",
    "    class_weight[i] = weights[i]\n",
    "print('class_weight:',class_weight)\n",
    "class_weight = class_weight.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles_text = np.load('train_matrices_text.npz', mmap_mode='r')\n",
    "X_train_text=trainfiles_text['X_train_text'] \n",
    "del trainfiles_text\n",
    "\n",
    "valfiles_text = np.load('val_matrices_text.npz', mmap_mode='r')\n",
    "X_val_text=valfiles_text['X_val_text']\n",
    "del valfiles_text\n",
    "\n",
    "testfiles_text = np.load('test_matrices_text.npz', mmap_mode='r')\n",
    "X_test_text=testfiles_text['X_test_text']\n",
    "del testfiles_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 0, 26, 6, 444302))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "full_plot_text (InputLayer)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2018900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 256)          234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 256)          394240    \n",
      "_________________________________________________________________\n",
      "soft_attention_concat_1 (Sof (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "new_features_text (Dense)    (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 36873     \n",
      "=================================================================\n",
      "Total params: 13,703,773\n",
      "Trainable params: 13,702,749\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/paulo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 66s 23ms/step - loss: 0.4957 - acc: 0.7791 - val_loss: 0.4525 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45249, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.4395 - acc: 0.8026 - val_loss: 0.4364 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45249 to 0.43639, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 61s 21ms/step - loss: 0.4060 - acc: 0.8158 - val_loss: 0.4132 - val_acc: 0.8057\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43639 to 0.41318, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 61s 21ms/step - loss: 0.3858 - acc: 0.8233 - val_loss: 0.4114 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.41318 to 0.41137, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3685 - acc: 0.8336 - val_loss: 0.3837 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41137 to 0.38372, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3623 - acc: 0.8334 - val_loss: 0.3824 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.38372 to 0.38238, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3490 - acc: 0.8412 - val_loss: 0.3707 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.38238 to 0.37070, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3421 - acc: 0.8447 - val_loss: 0.3694 - val_acc: 0.8304\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37070 to 0.36939, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3313 - acc: 0.8491 - val_loss: 0.3679 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.36939 to 0.36785, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3206 - acc: 0.8563 - val_loss: 0.3725 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.3131 - acc: 0.8601 - val_loss: 0.3673 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36785 to 0.36732, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2995 - acc: 0.8673 - val_loss: 0.3678 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2914 - acc: 0.8700 - val_loss: 0.3621 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36732 to 0.36212, saving model to tensorflow/tmp/weigths_mir_classifier_text.hdf5\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2824 - acc: 0.8757 - val_loss: 0.3760 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2752 - acc: 0.8800 - val_loss: 0.3788 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2622 - acc: 0.8849 - val_loss: 0.3744 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2560 - acc: 0.8884 - val_loss: 0.3694 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "2874/2874 [==============================] - 60s 21ms/step - loss: 0.2367 - acc: 0.8982 - val_loss: 0.3729 - val_acc: 0.8345\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "773/773 [==============================] - 4s 5ms/step\n",
      "[0.3692884720466732, 0.831105372575708]\n",
      "Action          0.6929\n",
      "Adventure       0.6964\n",
      "Comedy          0.7620\n",
      "Crime           0.5738\n",
      "Drama           0.7800\n",
      "Horror          0.6365\n",
      "Romance         0.4824\n",
      "SciFi           0.5296\n",
      "Thriller        0.4402\n",
      "macro           0.6216\n",
      "micro           0.6713\n",
      "weighted        0.6720\n",
      "('End:', datetime.datetime(2018, 6, 4, 0, 26, 6, 444302))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    # Branch 1 - RNN for text (Full Plot)\n",
    "    sequence_input = Input(shape=(max_sentence_len,), dtype='int32', name='full_plot_text')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    lstm = Bidirectional(LSTM(lstm_size, return_sequences=True, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n",
    "    lstm2 = Bidirectional(LSTM(lstm_size, return_sequences=True, recurrent_dropout=recurrent_dropout))(lstm)\n",
    "    attention_1 = SoftAttentionConcat()(lstm2)\n",
    "    batch1 = BatchNormalization()(attention_1)\n",
    "    branch_1 = Dense(1024, activation='relu')(batch1)\n",
    "    branch_1 = Dropout(0.5)(branch_1)\n",
    "        \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = branch_1\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_text')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    text_model = Model(inputs=[sequence_input], outputs=[predictions]) \n",
    "    \n",
    "    text_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_text.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    text_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = text_model.fit([X_train_text], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([X_val_text], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    text_model.save('MovieGenreClassifier_text.h5')\n",
    "    \n",
    "    y_pred_test = text_model.predict([X_test_text]) #\n",
    "    print(text_model.evaluate([X_test_text], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_text'\n",
    "    text_intermediate_layer_model = Model(inputs=[sequence_input],\n",
    "                                 outputs=text_model.get_layer(layer_name).output)\n",
    "    text_intermediate_output_train = text_intermediate_layer_model.predict([X_train_text])\n",
    "    text_intermediate_output_val = text_intermediate_layer_model.predict([X_val_text])\n",
    "    text_intermediate_output_test = text_intermediate_layer_model.predict([X_test_text])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release memory\n",
    "del X_train_text\n",
    "del X_val_text\n",
    "del X_test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poster NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "train_poster_images=trainfiles['train_poster_images']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "val_poster_images=valfiles['val_poster_images']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "test_poster_images=testfiles['test_poster_images']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 0, 45, 11, 847709))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "poster_image (InputLayer)    (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 222, 222, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 222, 222, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "new_features_poster (Dense)  (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 36873     \n",
      "=================================================================\n",
      "Total params: 10,572,853\n",
      "Trainable params: 10,572,783\n",
      "Non-trainable params: 70\n",
      "_________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 25s 9ms/step - loss: 0.4924 - acc: 0.7852 - val_loss: 0.4617 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46170, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4651 - acc: 0.7951 - val_loss: 0.4591 - val_acc: 0.8128\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46170 to 0.45915, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4601 - acc: 0.7975 - val_loss: 0.4486 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45915 to 0.44858, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4573 - acc: 0.7997 - val_loss: 0.4438 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44858 to 0.44379, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4552 - acc: 0.7997 - val_loss: 0.4500 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4530 - acc: 0.8022 - val_loss: 0.4436 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44379 to 0.44363, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4498 - acc: 0.7999 - val_loss: 0.4343 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.44363 to 0.43428, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4495 - acc: 0.8046 - val_loss: 0.4432 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4481 - acc: 0.8023 - val_loss: 0.4295 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.43428 to 0.42946, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4462 - acc: 0.8042 - val_loss: 0.4333 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4456 - acc: 0.8044 - val_loss: 0.4324 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4428 - acc: 0.8057 - val_loss: 0.4260 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.42946 to 0.42600, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4442 - acc: 0.8051 - val_loss: 0.4255 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.42600 to 0.42549, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4432 - acc: 0.8052 - val_loss: 0.4291 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4403 - acc: 0.8056 - val_loss: 0.4316 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4409 - acc: 0.8062 - val_loss: 0.4182 - val_acc: 0.8217\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.42549 to 0.41820, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4403 - acc: 0.8069 - val_loss: 0.4227 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4378 - acc: 0.8080 - val_loss: 0.4325 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4380 - acc: 0.8058 - val_loss: 0.4217 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4363 - acc: 0.8080 - val_loss: 0.4213 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4337 - acc: 0.8084 - val_loss: 0.4170 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.41820 to 0.41704, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 22/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4327 - acc: 0.8099 - val_loss: 0.4173 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4337 - acc: 0.8088 - val_loss: 0.4179 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4316 - acc: 0.8092 - val_loss: 0.4166 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.41704 to 0.41658, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 25/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4319 - acc: 0.8091 - val_loss: 0.4169 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4314 - acc: 0.8088 - val_loss: 0.4164 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.41658 to 0.41644, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 27/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4305 - acc: 0.8091 - val_loss: 0.4174 - val_acc: 0.8265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4314 - acc: 0.8093 - val_loss: 0.4163 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.41644 to 0.41635, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 29/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4305 - acc: 0.8096 - val_loss: 0.4156 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.41635 to 0.41556, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 30/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4312 - acc: 0.8085 - val_loss: 0.4166 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4310 - acc: 0.8104 - val_loss: 0.4173 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4304 - acc: 0.8107 - val_loss: 0.4164 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4316 - acc: 0.8108 - val_loss: 0.4146 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.41556 to 0.41463, saving model to tensorflow/tmp/weigths_mir_classifier_poster.hdf5\n",
      "Epoch 34/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4322 - acc: 0.8086 - val_loss: 0.4150 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4291 - acc: 0.8109 - val_loss: 0.4161 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4307 - acc: 0.8100 - val_loss: 0.4159 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4275 - acc: 0.8110 - val_loss: 0.4154 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/50\n",
      "2874/2874 [==============================] - 22s 8ms/step - loss: 0.4284 - acc: 0.8113 - val_loss: 0.4155 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "773/773 [==============================] - 2s 3ms/step\n",
      "[0.42387731340539103, 0.8160126643378244]\n",
      "Action          0.2980\n",
      "Adventure       0.3318\n",
      "Comedy          0.7637\n",
      "Crime           0.2662\n",
      "Drama           0.6274\n",
      "Horror          0.2986\n",
      "Romance         0.3572\n",
      "SciFi           0.1748\n",
      "Thriller        0.3549\n",
      "macro           0.3858\n",
      "micro           0.5355\n",
      "weighted        0.4863\n",
      "('End:', datetime.datetime(2018, 6, 4, 0, 45, 11, 847709))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "                \n",
    "    # Branch 2 - CNN for images (Poster)\n",
    "    '''\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    # Freeze Inception's weights - we don't want to train these\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # add a fully connected layer after Inception - we do want to train these\n",
    "    branch2 = base_model.output               \n",
    "    '''\n",
    "    poster_input = Input(shape=(width, height, depth), name='poster_image')   \n",
    "    branch2 = BatchNormalization()(poster_input)\n",
    "    branch2 = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    branch2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    branch2 = BatchNormalization()(branch2)\n",
    "    branch2 = MaxPooling2D(pool_size=(2, 2))(branch2)    \n",
    "    branch2 = Dropout(0.25)(branch2)               \n",
    "    branch2 = GlobalAveragePooling2D()(branch2)\n",
    "    branch2 = Dense(1024, activation='relu')(branch2)\n",
    "    branch2 = Dropout(0.5)(branch2)  \n",
    "    \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = branch2\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_poster')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    poster_model = Model(inputs=[poster_input], outputs=[predictions]) \n",
    "    \n",
    "    poster_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_poster.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    poster_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = poster_model.fit([train_poster_images], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([val_poster_images], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    poster_model.save('MovieGenreClassifier_poster.h5')\n",
    "    \n",
    "    y_pred_test = poster_model.predict([test_poster_images]) #\n",
    "    print(poster_model.evaluate([test_poster_images], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_poster'\n",
    "    poster_intermediate_layer_model = Model(inputs=[poster_input],\n",
    "                                 outputs=poster_model.get_layer(layer_name).output)\n",
    "    poster_intermediate_output_train = poster_intermediate_layer_model.predict([train_poster_images])\n",
    "    poster_intermediate_output_val = poster_intermediate_layer_model.predict([val_poster_images])\n",
    "    poster_intermediate_output_test = poster_intermediate_layer_model.predict([test_poster_images])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release memory\n",
    "del train_poster_images\n",
    "del val_poster_images\n",
    "del test_poster_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio NN / Low Level NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "train_spectograms=trainfiles['train_spectograms']\n",
    "train_X=trainfiles['train_X']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "val_spectograms=valfiles['val_spectograms']\n",
    "val_X=valfiles['val_X']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "testfiles.files\n",
    "test_spectograms=testfiles['test_spectograms']\n",
    "test_X=testfiles['test_X'] \n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 7, 57, 50, 483228))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:45: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "audio_spectrogram (InputLayer)  (None, 2, 44100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trainable_stft (Melspectrogram) (None, 64, 345, 2)   74304       audio_spectrogram[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "additive_noise_2 (AdditiveNoise (None, 64, 345, 2)   0           trainable_stft[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normalization2d_2 (Normalizatio (None, 64, 345, 2)   0           additive_noise_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 345, 32)  608         normalization2d_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 62, 343, 32)  9248        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 62, 343, 32)  128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 31, 171, 32)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 31, 171, 32)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 32)           0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1024)         33792       global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "low_level_features (InputLayer) (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1024)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4)            16          low_level_features[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 1028)         0           dropout_13[0][0]                 \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2048)         2107392     merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 2048)         0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "new_features_audio (Dense)      (None, 4096)         8392704     dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 4096)         0           new_features_audio[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 9)            36873       dropout_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,655,065\n",
      "Trainable params: 10,580,689\n",
      "Non-trainable params: 74,376\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 13s 5ms/step - loss: 0.4918 - acc: 0.7818 - val_loss: 0.5412 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54120, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4775 - acc: 0.7887 - val_loss: 0.5320 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54120 to 0.53202, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4770 - acc: 0.7876 - val_loss: 0.5159 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53202 to 0.51589, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4738 - acc: 0.7883 - val_loss: 0.5083 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51589 to 0.50830, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4733 - acc: 0.7910 - val_loss: 0.4981 - val_acc: 0.7977\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50830 to 0.49814, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4745 - acc: 0.7902 - val_loss: 0.4939 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49814 to 0.49392, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 12s 4ms/step - loss: 0.4739 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49392 to 0.48620, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 12s 4ms/step - loss: 0.4740 - acc: 0.7898 - val_loss: 0.4855 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48620 to 0.48546, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4725 - acc: 0.7927 - val_loss: 0.4820 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48546 to 0.48203, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4720 - acc: 0.7910 - val_loss: 0.4789 - val_acc: 0.7891\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48203 to 0.47887, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4723 - acc: 0.7926 - val_loss: 0.4731 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47887 to 0.47312, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4718 - acc: 0.7914 - val_loss: 0.4688 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47312 to 0.46879, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4714 - acc: 0.7908 - val_loss: 0.4676 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.46879 to 0.46755, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4704 - acc: 0.7937 - val_loss: 0.4678 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4707 - acc: 0.7904 - val_loss: 0.4648 - val_acc: 0.8051\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.46755 to 0.46481, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4693 - acc: 0.7930 - val_loss: 0.4625 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.46481 to 0.46251, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4697 - acc: 0.7920 - val_loss: 0.4629 - val_acc: 0.7986\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4691 - acc: 0.7918 - val_loss: 0.4618 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.46251 to 0.46184, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 19/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4702 - acc: 0.7935 - val_loss: 0.4601 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.46184 to 0.46013, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 20/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4694 - acc: 0.7944 - val_loss: 0.4548 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.46013 to 0.45484, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 21/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4681 - acc: 0.7947 - val_loss: 0.4546 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.45484 to 0.45460, saving model to tensorflow/tmp/weigths_mir_classifier_audio.hdf5\n",
      "Epoch 22/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4675 - acc: 0.7942 - val_loss: 0.4559 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4668 - acc: 0.7944 - val_loss: 0.4599 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4670 - acc: 0.7945 - val_loss: 0.4556 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4659 - acc: 0.7957 - val_loss: 0.4559 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "2874/2874 [==============================] - 11s 4ms/step - loss: 0.4664 - acc: 0.7931 - val_loss: 0.4551 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "773/773 [==============================] - 1s 2ms/step\n",
      "[0.4800695609614581, 0.7879833397088045]\n",
      "Action          0.2122\n",
      "Adventure       0.1397\n",
      "Comedy          0.3907\n",
      "Crime           0.1565\n",
      "Drama           0.5097\n",
      "Horror          0.1009\n",
      "Romance         0.1578\n",
      "SciFi           0.0737\n",
      "Thriller        0.1669\n",
      "macro           0.2120\n",
      "micro           0.3628\n",
      "weighted        0.2986\n",
      "('End:', datetime.datetime(2018, 6, 4, 7, 57, 50, 483228))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    # Branch 3 - CNN for spectrogram image (Audio)\n",
    "    audio_inputs = Input(shape=audio_input_shape, name='audio_spectrogram')\n",
    "    #A mel-spectrogram layer    \n",
    "    branch3 = Melspectrogram(n_dft=256, n_hop=128,\n",
    "                             padding='same', sr=sr, n_mels=64,\n",
    "                             fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                             return_decibel_melgram=False, trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='trainable_stft')(audio_inputs)\n",
    "    # Maybe some additive white noise.\n",
    "    branch3 = AdditiveNoise(power=0.2)(branch3)\n",
    "    # If you wanna normalise it per-frequency\n",
    "    branch3 = Normalization2D(str_axis='freq')(branch3) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "    #CNN\n",
    "    branch3 = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    branch3 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    branch3 = BatchNormalization()(branch3)\n",
    "    branch3 = MaxPooling2D(pool_size=(2, 2))(branch3)\n",
    "    branch3 = Dropout(0.25)(branch3)   \n",
    "    branch3 = GlobalAveragePooling2D()(branch3)\n",
    "    branch3 = Dense(1024, activation='relu')(branch3)\n",
    "    branch3 = Dropout(0.5)(branch3)\n",
    "        \n",
    "    #Branch5 - Low Level Features\n",
    "    low_level_inputs = Input(shape=(low_level_features,), name='low_level_features')\n",
    "    branch5 = BatchNormalization()(low_level_inputs)\n",
    "    \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([branch3, branch5], mode='concat')\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_audio')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    audio_model = Model(inputs=[audio_inputs, low_level_inputs], outputs=[predictions]) \n",
    "    \n",
    "    audio_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_audio.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    audio_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = audio_model.fit([train_spectograms,train_X], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([val_spectograms,val_X ], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    audio_model.save('MovieGenreClassifier_audio.h5')\n",
    "    \n",
    "    y_pred_test = audio_model.predict([test_spectograms, test_X]) #\n",
    "    print(audio_model.evaluate([test_spectograms, test_X], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_audio'\n",
    "    audio_intermediate_layer_model = Model(inputs=[audio_inputs, low_level_inputs],\n",
    "                                 outputs=audio_model.get_layer(layer_name).output)\n",
    "    audio_intermediate_output_train = audio_intermediate_layer_model.predict([train_spectograms, train_X])\n",
    "    audio_intermediate_output_val = audio_intermediate_layer_model.predict([val_spectograms, val_X])\n",
    "    audio_intermediate_output_test = audio_intermediate_layer_model.predict([test_spectograms, test_X])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_spectograms\n",
    "del train_X\n",
    "\n",
    "del val_spectograms\n",
    "del val_X\n",
    "\n",
    "del test_spectograms\n",
    "del test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "train_lmtd_features=trainfiles['train_lmtd_features']\n",
    "del trainfiles\n",
    "\n",
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "val_lmtd_features=valfiles['val_lmtd_features']\n",
    "del valfiles\n",
    "\n",
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "test_lmtd_features=testfiles['test_lmtd_features']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 8, 34, 2, 735886))\n",
      "WARNING:tensorflow:From /home/paulo/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lmtd9_features (InputLayer)  (None, 240, 2048)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 240, 2048)         8192      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 238, 384)          2359680   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 238, 384)          1536      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 238, 384)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2048)              788480    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "new_features_video (Dense)   (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 9)                 36873     \n",
      "=================================================================\n",
      "Total params: 15,783,817\n",
      "Trainable params: 15,778,953\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 158s 55ms/step - loss: 0.5954 - acc: 0.7550 - val_loss: 0.5101 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51008, saving model to tensorflow/tmp/weigths_mir_classifier_video.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 177s 62ms/step - loss: 0.5099 - acc: 0.7739 - val_loss: 0.5565 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 126s 44ms/step - loss: 0.4969 - acc: 0.7823 - val_loss: 0.5629 - val_acc: 0.8137\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 19s 7ms/step - loss: 0.4893 - acc: 0.7826 - val_loss: 0.5688 - val_acc: 0.8057\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 19s 7ms/step - loss: 0.4805 - acc: 0.7847 - val_loss: 0.5649 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 19s 7ms/step - loss: 0.4730 - acc: 0.7874 - val_loss: 0.5518 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "773/773 [==============================] - 15s 20ms/step\n",
      "[0.5559734142304701, 0.8177375384783528]\n",
      "Action          0.6047\n",
      "Adventure       0.3882\n",
      "Comedy          0.7741\n",
      "Crime           0.2750\n",
      "Drama           0.6021\n",
      "Horror          0.2443\n",
      "Romance         0.3295\n",
      "SciFi           0.2123\n",
      "Thriller        0.2963\n",
      "macro           0.4140\n",
      "micro           0.5444\n",
      "weighted        0.5118\n",
      "('End:', datetime.datetime(2018, 6, 4, 8, 34, 2, 735886))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "            \n",
    "    #Branch 4 - LTMD9 Features                    \n",
    "    lmtd_inputs = Input(shape=(time_steps, nb_features), name='lmtd9_features')    \n",
    "    branch4 = BatchNormalization()(lmtd_inputs) \n",
    "    branch4 = Convolution1D(conv_filters, kernel_size=3)(branch4)\n",
    "    branch4 = BatchNormalization()(branch4)\n",
    "    branch4 = Activation('relu')(branch4)\n",
    "    branch4 = GlobalMaxPooling1D()(branch4)\n",
    "    branch4 = Dropout(dropout)(branch4)\n",
    "    branch4 = Dense(2048, activation='relu')(branch4)\n",
    "    branch4 = Dropout(0.5)(branch4)\n",
    "       \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = branch4\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features_video')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    video_model = Model(inputs=[lmtd_inputs], outputs=[predictions]) \n",
    "    \n",
    "    video_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_video.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    video_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = video_model.fit([train_lmtd_features], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([val_lmtd_features], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    video_model.save('MovieGenreClassifier_video.h5')\n",
    "    \n",
    "    y_pred_test = video_model.predict([test_lmtd_features]) #\n",
    "    print(video_model.evaluate([test_lmtd_features], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for Late NN after\n",
    "    layer_name = 'new_features_video'\n",
    "    video_intermediate_layer_model = Model(inputs=[lmtd_inputs],\n",
    "                                 outputs=video_model.get_layer(layer_name).output)\n",
    "    video_intermediate_output_train = video_intermediate_layer_model.predict([train_lmtd_features])\n",
    "    video_intermediate_output_val = video_intermediate_layer_model.predict([val_lmtd_features])\n",
    "    video_intermediate_output_test = video_intermediate_layer_model.predict([test_lmtd_features])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_lmtd_features\n",
    "\n",
    "del val_lmtd_features\n",
    "\n",
    "del test_lmtd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "text_intermediate_output_train = np.reshape(text_intermediate_output_train,(text_intermediate_output_train.shape[0],text_intermediate_output_train.shape[1]))\n",
    "text_intermediate_output_val = np.reshape(text_intermediate_output_val,(poster_intermediate_output_val.shape[0],poster_intermediate_output_val.shape[1]))\n",
    "text_intermediate_output_test = np.reshape(text_intermediate_output_test,(poster_intermediate_output_test.shape[0],poster_intermediate_output_test.shape[1]))\n",
    "print(text_intermediate_output_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "poster_intermediate_output_train = np.reshape(poster_intermediate_output_train,(poster_intermediate_output_train.shape[0],poster_intermediate_output_train.shape[1]))\n",
    "poster_intermediate_output_val = np.reshape(poster_intermediate_output_val,(poster_intermediate_output_val.shape[0],poster_intermediate_output_val.shape[1]))\n",
    "poster_intermediate_output_test = np.reshape(poster_intermediate_output_test,(poster_intermediate_output_test.shape[0],poster_intermediate_output_test.shape[1]))\n",
    "print(poster_intermediate_output_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "audio_intermediate_output_train = np.reshape(audio_intermediate_output_train,(audio_intermediate_output_train.shape[0],audio_intermediate_output_train.shape[1]))\n",
    "audio_intermediate_output_val = np.reshape(audio_intermediate_output_val,(audio_intermediate_output_val.shape[0],audio_intermediate_output_val.shape[1]))\n",
    "audio_intermediate_output_test = np.reshape(audio_intermediate_output_test,(audio_intermediate_output_test.shape[0],audio_intermediate_output_test.shape[1]))\n",
    "print(audio_intermediate_output_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2874, 4096)\n"
     ]
    }
   ],
   "source": [
    "video_intermediate_output_train = np.reshape(video_intermediate_output_train,(video_intermediate_output_train.shape[0], video_intermediate_output_train.shape[1]))\n",
    "video_intermediate_output_val = np.reshape(video_intermediate_output_val,(video_intermediate_output_val.shape[0], video_intermediate_output_val.shape[1]))\n",
    "video_intermediate_output_test = np.reshape(video_intermediate_output_test,(video_intermediate_output_test.shape[0], video_intermediate_output_test.shape[1]))\n",
    "print(video_intermediate_output_train.shape)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late fusion NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 6, 4, 20, 53, 2, 836005))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "            \n",
    "    text_inputs = Input(shape=(4096,), name='text_features')    \n",
    "    poster_inputs = Input(shape=(4096,), name='poster_features')    \n",
    "    audio_inputs = Input(shape=(4096,), name='audio_features')    \n",
    "    video_inputs = Input(shape=(4096,), name='video_features')    \n",
    "           \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([text_inputs, poster_inputs, audio_inputs, video_inputs], mode='concat')\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    full_model = Model(inputs=[text_inputs, poster_inputs,  audio_inputs, video_inputs], outputs=[predictions]) \n",
    "    \n",
    "    full_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate/10)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier_full.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    full_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = full_model.fit([text_intermediate_output_train, poster_intermediate_output_train, audio_intermediate_output_train, video_intermediate_output_train], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([text_intermediate_output_val, poster_intermediate_output_val, audio_intermediate_output_val, video_intermediate_output_val], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    full_model.save('MovieGenreClassifier_full.h5')\n",
    "    \n",
    "    y_pred_test = full_model.predict([text_intermediate_output_test, poster_intermediate_output_test, audio_intermediate_output_test, video_intermediate_output_test]) #\n",
    "    print(full_model.evaluate([text_intermediate_output_test, poster_intermediate_output_test, audio_intermediate_output_test, video_intermediate_output_test], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)                                   \n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

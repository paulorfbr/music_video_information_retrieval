{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda numpy==1.14.0 --yes\n",
    "#!conda install -c conda-forge keras --yes\n",
    "#!conda install -c anaconda tensorflow-gpu --yes\n",
    "#!conda install -c anaconda gensim --yes\n",
    "#!pip install kapre\n",
    "#!pip install tensorboard\n",
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import sys\n",
    "import skvideo.io\n",
    "from skvideo.measure import scenedet\n",
    "import json\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy import sparse\n",
    "import urllib\n",
    "from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time as t\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "sys.path.append('lmtd')\n",
    "from lmtd9 import LMTD\n",
    "from lmtd9 import database as db\n",
    "from lmtd9 import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa for audio\n",
    "import librosa\n",
    "# And the display module for visualization\n",
    "import librosa.display\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "from sklearn import preprocessing\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffmpeg_load_audio import ffmpeg_load_audio\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "import urlparse, os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import operator\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU\n",
    "from keras import applications\n",
    "from keras.layers import GlobalAveragePooling2D, merge, Input\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, BatchNormalization\n",
    "from keras.layers import Input, Convolution1D, GlobalMaxPooling1D, merge, Dense, Dropout\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n_cpus:', 8)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "gaussian_noise_lstm = 0.01\n",
    "lstm_size = 128\n",
    "gru_size = 64\n",
    "recurrent_dropout = 0.2\n",
    "dropout_cnn = 0.5\n",
    "gaussian_noise_cnn = 0.1\n",
    "model_output_path = \"/home/paulo/mestrado/MovieGenreClassifier.model\"\n",
    "validation_split = 0.2\n",
    "test_split = 0.2\n",
    "regularizer_lambda = 0.01\n",
    "GLOVE_DIR = \"/home/paulo/mestrado/glove.6B/\"\n",
    "min_word_frequency_word2vec = 5,\n",
    "embed_size_word2vec = 100\n",
    "context_window_word2vec = 10\n",
    "MAX_NB_WORDS = 50000\n",
    "max_sentence_len = 100\n",
    "maxlen = max_sentence_len\n",
    "min_sentence_length = 15\n",
    "sr = 44100 #sampling rate 44.1KHz\n",
    "audio_channels = 1 #mono\n",
    "width = 224 #224 for VGG16,VGG19,ResNet50 (299 for inceptionV3)\n",
    "height = 224  #224 for VGG16,VGG19,ResNet50 (299 for inceptionV3)  \n",
    "depth = 3 #rgb\n",
    "audio_channels = 2 #stereo\n",
    "audio_input_shape = (audio_channels, sr) #2-channels - 44.1KHz\n",
    "n_mels=64 #Mel Spectogram Size\n",
    "duration=60 #Uniform time in seconds for trailer music (60=1 minute)\n",
    "time_steps = 240\n",
    "low_level_features = 4 #low level features\n",
    "nb_features = 2048 #ltmd features\n",
    "nb_classes = 9\n",
    "conv_filters = 384\n",
    "dropout = 0.5\n",
    "max_epochs = 50\n",
    "n_mels=128\n",
    "n_hops=256\n",
    "n_mfcc=40\n",
    "nb_frames=1000\n",
    "num_classes=9\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()    \n",
    "print('n_cpus:',n_cpus)      \n",
    "number_gpus = 1\n",
    "number_cpus = n_cpus-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMTD_PATH = '/home/paulo/mestrado/lmtd'    \n",
    "lmtd = LMTD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = os.path.join(LMTD_PATH, 'features', 'lmtd9_resnet152.pickle')\n",
    "with open(features_path, 'rb') as fp:\n",
    "    feature_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(all_data):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(all_data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_glove_embeddings():\n",
    "    embeddings_index = {}\n",
    "    glove_file = 'glove.6B.{}d.txt'.format(embed_size_word2vec) #need to make both to match (embed_size_word2vec and globe index)\n",
    "    f = open(os.path.join(GLOVE_DIR, glove_file))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embeddings_matrices(embeddings_index, word_index):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, embed_size_word2vec))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embedding_layer(word_index, embedding_matrix):\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                embed_size_word2vec,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sentence_len,\n",
    "                                trainable=True)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHistograms(img):\n",
    "    hsv_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    hue, sat, ilum = hsv_image[:,:,0], hsv_image[:,:,1], hsv_image[:,:,2]    \n",
    "    hist_hue, bin_edges_hue = np.histogram(np.ndarray.flatten(hue), bins=8)\n",
    "    hist_sat, bin_edges_sat = np.histogram(np.ndarray.flatten(sat), bins=4)\n",
    "    hist_ilum, bin_edges_ilum = np.histogram(np.ndarray.flatten(ilum), bins=4)\n",
    "    return np.hstack((hist_hue, hist_sat, hist_ilum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_intersection(previousHist, currentHist):\n",
    "    s_i = 0\n",
    "    #for each bin \n",
    "    #print(currentHist.shape[0])\n",
    "    for j in range(currentHist.shape[0]):\n",
    "        min_bin_s_i = min(currentHist[j],previousHist[j])\n",
    "        #print('minimum from bin {}: {}'.format(j,min_bin_s_i))\n",
    "        min_sum_prev = sum(previousHist)\n",
    "        min_sum_cur = sum(currentHist)\n",
    "        min_both = min(min_sum_prev, min_sum_cur) #normalize (maximum can be 1)\n",
    "        s_i+= (min_bin_s_i/float(min_both)) \n",
    "    return s_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradE(sNext, sCurrent):\n",
    "    return sNext-sCurrent\n",
    "\n",
    "def gradW(sPrevious, sCurrent):\n",
    "    return sPrevious-sCurrent\n",
    "\n",
    "def gaussianKernel(x, k=0.1):\n",
    "    return np.exp(-(x/k)**2)\n",
    "\n",
    "def smoothed_s(s_i_previous, s_i, s_i_next, lamb=0.1, k=0.1):\n",
    "    gEi = gradE(s_i_next, s_i)\n",
    "    gWi = gradW(s_i_previous, s_i)\n",
    "    CEt = gaussianKernel(np.absolute(gEi))\n",
    "    CWt = gaussianKernel(np.absolute(gWi))\n",
    "    stnext_i = s_i + lamb*(CEt*gEi + CWt * gWi)\n",
    "    return stnext_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = 'images/paulo_meire.png'\n",
    "img = cv2.imread(target_image)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image2 = 'images/paulo_meire2.png'\n",
    "img2 = cv2.imread(target_image2)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = createHistograms(img)\n",
    "plt.hist(hist1)\n",
    "print(hist1)\n",
    "hist2 = createHistograms(img2)\n",
    "plt.hist(hist2)\n",
    "print(hist2)\n",
    "print(histogram_intersection(hist1, hist2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_variance(videodata, smooth_scene_cuts):\n",
    "    Ls = []\n",
    "    Us = []\n",
    "    Vs = []\n",
    "    for frame in smooth_scene_cuts:\n",
    "        luv_image = cv2.cvtColor(frame, cv2.COLOR_RGB2Luv)\n",
    "        L, u, v = luv_image[:,:,0], luv_image[:,:,1], luv_image[:,:,2]\n",
    "        Ls.append(L)\n",
    "        Us.append(u)\n",
    "        Vs.append(v)        \n",
    "    Luv = np.stack((np.array(Ls).flatten(), np.array(Us).flatten(), np.array(Vs).flatten()), axis=0)\n",
    "    pCov = np.cov(Luv)\n",
    "    #print('covLuv:', pCov)\n",
    "    #print('varL:',np.var(np.array(Ls)))\n",
    "    #print('varU:',np.var(np.array(Us)))\n",
    "    #print('varV:',np.var(np.array(Vs)))\n",
    "    totalColorVariance = np.linalg.det(pCov)\n",
    "    #print('totalColorVariance:', totalColorVariance)\n",
    "    return totalColorVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailer_ids = lmtd.train_ids\n",
    "comedy_id = None\n",
    "horror_id = None\n",
    "action_id = None\n",
    "\n",
    "for _id in trailer_ids:\n",
    "    # Returns a dictionary in which keys are the queried trailer_ids\n",
    "    movie_data = lmtd.get_data_by_trailer_ids(_id)\n",
    "    #print(movie_data)\n",
    "    for metadata in movie_data.keys():\n",
    "        print('trailer id : ', metadata)\n",
    "        print('title db : ', movie_data[metadata]['Title'])\n",
    "        print('genres db : ', movie_data[metadata]['Genre'])\n",
    "        print('labels       : ', lmtd.train_labels[int(metadata)])\n",
    "        print('re-converted : ', lmtd.binary_label_to_genre(lmtd.train_labels[int(metadata)])[0])\n",
    "        if comedy_id is not None and horror_id is not None and action_id is not None:\n",
    "            break  \n",
    "        elif ('Comedy' in movie_data[metadata]['Genre']):\n",
    "            comedy_id = _id\n",
    "        elif ('Horror' in movie_data[metadata]['Genre']):    \n",
    "            horror_id = _id            \n",
    "        elif ('Action' in movie_data[metadata]['Genre']):    \n",
    "            action_id = _id \n",
    "print('Comedy:',comedy_id)\n",
    "print('Horror:',horror_id)\n",
    "print('Action:',action_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lighting Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighting_key(videodata, smooth_scene_cuts):    \n",
    "    hsv_images = []\n",
    "    for frame in smooth_scene_cuts:\n",
    "        hsv_image = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "        hsv_images.append(hsv_image)        \n",
    "    meanHsv = np.mean(hsv_images)\n",
    "    stdHsv = np.std(hsv_images)\n",
    "    lightingkey = meanHsv * stdHsv\n",
    "    #print(lightingkey)\n",
    "    return lightingkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_content(videodata, smooth_scene_cuts, w=3):    \n",
    "    frameOld = smooth_scene_cuts[0]     \n",
    "    count_active_pixels = 0\n",
    "    count_all_pixels = 0\n",
    "    for currentFrame in smooth_scene_cuts:\n",
    "        Hx = cv2.Sobel(currentFrame,cv2.CV_64F,1,0,ksize=w)\n",
    "        #print('Hx:',Hx)\n",
    "        Ht = currentFrame - frameOld  #temporal derivative\n",
    "        #print('Ht:',Ht)\n",
    "        Hx2 = Hx * Hx\n",
    "        #print('Hx2:',Hx2)\n",
    "        Ht2 = Ht * Ht\n",
    "        #print('Ht2:',Ht2)\n",
    "        HxHt = Hx * Ht\n",
    "        #print('HxHt:',HxHt)\n",
    "        Jxx = sum(Hx2)\n",
    "        #print('Jxx:',Jxx)\n",
    "        Jxt = sum(HxHt)\n",
    "        #print('Jxt:',Jxt)\n",
    "        Jtt = sum(Ht2)\n",
    "        #print('Jtt:',Jtt)\n",
    "        #print(2*Jxt/(Jxx-Jxt))\n",
    "        theta = (1/2.0)*np.degrees(np.arctan(2*Jxt/(Jxx-Jxt)))\n",
    "        count_all_pixels+= theta.size  \n",
    "        #print(count_all_pixels)\n",
    "        aux = theta[~np.isnan(theta)]\n",
    "        aux = aux.flatten()\n",
    "        #print(aux)\n",
    "        count_active_pixels += aux[(0.0 < aux) | (aux < 10.0)].size\n",
    "        #print(count_active_pixels)\n",
    "        frameOld = currentFrame        \n",
    "    return count_active_pixels/float(count_all_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_filename = 'trailers/videos/'+comedy_id+'.mp4'\n",
    "horror_filename = 'trailers/videos/'+horror_id+'.mp4'\n",
    "action_filename = 'trailers/videos/'+action_id+'.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_videometadata = skvideo.io.ffprobe(comedy_filename)\n",
    "print(comedy_videometadata.keys())\n",
    "print(json.dumps(comedy_videometadata[\"video\"], indent=4))\n",
    "print(json.dumps(comedy_videometadata[\"audio\"], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_videodata = skvideo.io.vread(comedy_filename)\n",
    "print(comedy_videodata.shape)\n",
    "T, M, N, C = comedy_videodata.shape\n",
    "\n",
    "print(\"Number of frames: %d\" % (T,))\n",
    "print(\"Number of rows: %d\" % (M,))\n",
    "print(\"Number of cols: %d\" % (N,))\n",
    "print(\"Number of channels: %d\" % (C,))\n",
    "\n",
    "frame_rate = comedy_videometadata['video']['@avg_frame_rate']\n",
    "num_frames = np.int(comedy_videodata.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the \"luminance\" algorithm\n",
    "#comedy_scene_lum_idx = skvideo.measure.scenedet(comedy_videodata, method='histogram', parameter1=1.0)\n",
    "#comedy_scenecuts = comedy_videodata[comedy_scene_lum_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(comedy_scenecuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_detection(videodata):\n",
    "    #videodata = skvideo.io.vread(movie_filename)\n",
    "    #T, M, N, C = videodata.shape\n",
    "    #videometadata = skvideo.io.ffprobe(movie_filename)\n",
    "    #frame_rate = videometadata['video']['@avg_frame_rate']\n",
    "    #num_frames = np.int(videodata.shape[0])\n",
    "    #width = np.int(videometadata['video']['@width'])\n",
    "    #height = np.int(videometadata['video']['@height'])\n",
    "    numFrames, height, width, channels = videodata.shape\n",
    "    smooth_list = []\n",
    "    for t in range(2, numFrames-1):\n",
    "        beforePrevHist = createHistograms(videodata[t-2])\n",
    "        prevHist = createHistograms(videodata[t-1])\n",
    "        currHist = createHistograms(videodata[t])\n",
    "        nextHist = createHistograms(videodata[t+1])\n",
    "        s_i_prev = histogram_intersection(beforePrevHist,prevHist)\n",
    "        s_i_cur = histogram_intersection(prevHist,currHist)\n",
    "        s_i_next = histogram_intersection(currHist,nextHist)  \n",
    "        s_smooth = smoothed_s(s_i_prev, s_i_cur, s_i_next)\n",
    "        smooth_list.append(s_smooth)    \n",
    "    #if first derivative very near 0 (critical point) and second derivative > 0 (local minima)    \n",
    "    local_minima_index = argrelextrema(np.array(smooth_list), np.less)\n",
    "    #print(local_minima_index)\n",
    "    #intersects with normal histogram method ()\n",
    "    threshold_index = [i for i,v in enumerate(smooth_list) if v < 0.7]\n",
    "    cut_scene_index = sorted(list(set(local_minima_index[0]).intersection(threshold_index)))  \n",
    "    #print('Shots detected:', len(cut_scene_index))\n",
    "    #print('Index cut scenes:', cut_scene_index)\n",
    "    #plt.ylabel('Shot Detection')\n",
    "    #plt.xlabel('Frames')\n",
    "    #plt.show()   \n",
    "    smooth_scene_cuts = videodata[cut_scene_index]\n",
    "    return videodata, smooth_scene_cuts\n",
    "\n",
    "def average_shot_length(videodata, smooth_scene_cuts):\n",
    "    num_frames = np.int(videodata.shape[0])\n",
    "    return num_frames/float(len(smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Histogram Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for frame in comedy_scenecuts:\n",
    "    imgplot = plt.imshow(frame)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed Histogram Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "comedy_videodata, comedy_smooth_scene_cuts = shot_detection(comedy_videodata)\n",
    "for frame in comedy_smooth_scene_cuts:\n",
    "    imgplot = plt.imshow(frame)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "horror_videodata, horror_smooth_scene_cuts = shot_detection(horror_filename)\n",
    "for frame in horror_smooth_scene_cuts:\n",
    "    imgplot = plt.imshow(frame)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "action_videodata, action_smooth_scene_cuts = shot_detection(action_filename)\n",
    "for frame in action_smooth_scene_cuts:\n",
    "    imgplot = plt.imshow(frame)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Shot Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Comedy Average shot length:', average_shot_length(comedy_videodata, comedy_smooth_scene_cuts))\n",
    "#print('Horror Average shot length:', average_shot_length(horror_videodata, horror_smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Comedy Color Variance:', color_variance(comedy_videodata, comedy_smooth_scene_cuts))\n",
    "#print('Horror Color Variance:', color_variance(horror_videodata, horror_smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lighting Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High value - comedy\n",
    "### Low value - horror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Comedy Lighting Key:', lighting_key(comedy_videodata, comedy_smooth_scene_cuts))\n",
    "#print('Horror Lighting Key:', lighting_key(horror_videodata, horror_smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Comedy Motion Content:', motion_content(comedy_videodata, comedy_smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Horror Motion Content:', motion_content(horror_videodata, horror_smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Action Motion Content:', motion_content(action_videodata, action_smooth_scene_cuts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_path):    \n",
    "    y, sample_rate = librosa.load(audio_path)\n",
    "    S = librosa.feature.melspectrogram(y, sr=sample_rate, n_mels=n_mels)\n",
    "    log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    tempo_bpm, beats = librosa.beat.beat_track(y=y_percussive, sr=sample_rate)\n",
    "    stft = np.abs(librosa.stft(y))\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sample_rate)[0]\n",
    "    spec_bandwith = librosa.feature.spectral_bandwidth(y=y, sr=sample_rate)[0]\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=S, sr=sample_rate))\n",
    "    flatness = librosa.feature.spectral_flatness(y=y)[0]\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sample_rate)[0]\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    mfcc        = librosa.feature.mfcc(y=y, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    mfccs = np.mean(mfcc.T,axis=0)\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=y_harmonic,sr=sample_rate).T,axis=0)\n",
    "    hop_length = n_hops\n",
    "    oenv = librosa.onset.onset_strength(y=y, sr=sample_rate, hop_length=hop_length)\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sample_rate, hop_length=hop_length)\n",
    "    tempogram_ravel = np.ravel(tempogram)\n",
    "    # Compute global onset autocorrelation\n",
    "    ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n",
    "    ac_global = librosa.util.normalize(ac_global)   \n",
    "    return mfccs, chroma_stft, mel,contrast,tonnetz, tempo_bpm, spectral_centroid, spec_bandwith, flatness, rolloff, zero_crossing_rate, oenv, tempogram_ravel, ac_global\n",
    "\n",
    "def extract_audio_features_melspectrogram(audio_path, i): \n",
    "    y, sample_rate = librosa.load(audio_path, sr=sr, duration=duration, mono=True)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    if i==0:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(librosa.power_to_db(S,ref=np.max),y_axis='mel', x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel spectrogram')        \n",
    "    len_second = 1.0 # 1 second\n",
    "    src = y[:int(sr*len_second)]\n",
    "    src = src[np.newaxis, :]\n",
    "    input_shape = src.shape\n",
    "    print(input_shape)    \n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_audio_features(comedy_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(link, filelocation):\n",
    "    r = requests.get(link, stream=True)\n",
    "    with open(filelocation, 'wb') as f:\n",
    "        for chunk in r.iter_content(1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                \n",
    "def createNewDownloadThread(link, filelocation):\n",
    "    download_thread = threading.Thread(target=download, args=(link,filelocation))\n",
    "    download_thread.start()\n",
    "\n",
    "def download_poster(movie_data, _id, folder): \n",
    "    time.sleep(0.1)\n",
    "    poster_url = movie_data[_id]['Poster']\n",
    "    print('Poster URL:', poster_url)\n",
    "    filename = poster_url[poster_url.rfind(\"/\")+1:]\n",
    "    print('filename:', filename)\n",
    "    file_str = \"/home/paulo/mestrado/posters/\" + folder + \"/\" + _id + \".jpg\"\n",
    "    print('file:', file_str)    \n",
    "    createNewDownloadThread(poster_url, file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistics(feature_values):    \n",
    "    _mean = np.mean(feature_values)\n",
    "    _var = np.var(feature_values)\n",
    "    _skew = scipy.stats.skew(feature_values)\n",
    "    _kurtosis = scipy.stats.kurtosis(feature_values)\n",
    "    _median = np.median(feature_values)\n",
    "    _min = np.min(feature_values)\n",
    "    _max = np.max(feature_values)\n",
    "    return _mean, _var, _skew, _kurtosis, _median, _min, _max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, maxlen):\n",
    "    left = 0 if maxlen < len(seq) else (maxlen - len(seq))\t\n",
    "    left_pad = np.zeros((left, len(seq[-1])))\n",
    "    pad_feat = np.concatenate([seq, left_pad], 0)[:maxlen]\n",
    "    return pad_feat\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def process_movie(movie_data, trailer_labels, _id, i, Y, features, Genres, FullPlots, spectograms, lmtd_features):        \n",
    "    trailer_id = _id\n",
    "    title_db = movie_data[_id]['Title']\n",
    "    genres_db = movie_data[_id]['Genre']\n",
    "    if 'FullPlot' in movie_data[_id].keys() and movie_data[_id]['FullPlot'] is not None:\n",
    "          full_plot = title_db + ' ' + movie_data[_id]['FullPlot']\n",
    "    else:\n",
    "          full_plot = title_db + ' ' + movie_data[_id]['Plot']    \n",
    "    labels = trailer_labels[i]                                \n",
    "    avg_shot_length = 0\n",
    "    color_var = 0\n",
    "    light_key = 0\n",
    "    mot_content = 0    \n",
    "    Y[i] = labels\n",
    "    Genres.append(genres_db)\n",
    "    #stemmize and lower\n",
    "    plot_stemmed = \"\"\n",
    "    tokens = word_tokenize(strip_accents(unicode(full_plot.encode('string_escape'))))\n",
    "    for w in tokens:        \n",
    "        if w.lower().strip() not in stopwords.words('english'):\n",
    "            #print(w.lower().strip())\n",
    "            w = strip_accents(unicode(w))\n",
    "            #stemmed_word = stemmer.stem(w)\n",
    "            plot_stemmed += ' ' + w #stemmed_word\n",
    "    full_plot = plot_stemmed.lower().strip()\n",
    "    print((i, trailer_id, title_db, genres_db, labels, full_plot))\n",
    "    FullPlots.append(full_plot)\n",
    "    filename = 'trailers/videos/'+trailer_id+'.mp4'    \n",
    "    '''    \n",
    "    feat = np.asarray(feature_dict[_id])\n",
    "    _pad_feat = pad_sequence(feat, maxlen=time_steps)    \n",
    "    print(_pad_feat.shape)\n",
    "    lmtd_features[i] = _pad_feat\n",
    "\n",
    "    try:\n",
    "            #videometadata = skvideo.io.ffprobe(filename)\n",
    "            #print(videometadata.keys())\n",
    "            videodata = skvideo.io.vread(filename, num_frames=nb_frames)            \n",
    "            #_, smooth_scene_cuts = shot_detection(videodata)\n",
    "            scene_lum_idx = skvideo.measure.scenedet(videodata, method='histogram', parameter1=1.0)\n",
    "            smooth_scene_cuts = videodata[scene_lum_idx]\n",
    "            avg_shot_length = average_shot_length(videodata, smooth_scene_cuts)                \n",
    "            color_var = color_variance(videodata, smooth_scene_cuts)\n",
    "            light_key = lighting_key(videodata, smooth_scene_cuts)\n",
    "            mot_content = motion_content(videodata, smooth_scene_cuts)                    \n",
    "            \n",
    "            features[i][0] = avg_shot_length\n",
    "            features[i][1] = color_var\n",
    "            features[i][2] = light_key\n",
    "            features[i][3] = mot_content   \n",
    "            del videodata\n",
    "            del smooth_scene_cuts                    \n",
    "    except Exception as e:\n",
    "        print(\"error video extraction: {0}\".format(e))\n",
    "        pass\n",
    "    '''\n",
    "    #try:\n",
    "    #        src = extract_audio_features_melspectrogram(filename, i)               \n",
    "    #        spectograms[i] = src\n",
    "            \n",
    "    '''\n",
    "            mfccs, chroma_stft, mel,contrast,tonnetz, tempo_bpm, spectral_centroid, spec_bandwith, flatness, rolloff, zero_crossing_rate, oenv, tempogram, ac_global = extract_audio_features(filename)                        \n",
    "            mfccs_mean, mfccs_var, mfccs_skew, mfccs_kurtosis, mfccs_median, mfccs_min, mfccs_max = extract_statistics(mfccs)\n",
    "            chroma_stft_mean, chroma_stft_var, chroma_stft_skew, chroma_stft_kurtosis, chroma_stft_median, chroma_stft_min, chroma_stft_max = extract_statistics(chroma_stft)        \n",
    "            mel_mean, mel_var, mel_skew, mel_kurtosis, mel_median, mel_min, mel_max = extract_statistics(mel)\n",
    "            contrast_mean, contrast_var, contrast_skew, contrast_kurtosis, contrast_median, contrast_min, contrast_max = extract_statistics(contrast)\n",
    "            tonnetz_mean, tonnetz_var, tonnetz_skew, tonnetz_kurtosis, tonnetz_median, tonnetz_min, tonnetz_max = extract_statistics(tonnetz)\n",
    "            spectral_centroid_mean, spectral_centroid_var, spectral_centroid_skew, spectral_centroid_kurtosis, spectral_centroid_median, spectral_centroid_min, spectral_centroid_max = extract_statistics(spectral_centroid)\n",
    "            spec_bandwith_mean, spec_bandwith_var, spec_bandwith_skew, spec_bandwith_kurtosis, spec_bandwith_median, spec_bandwith_min, spec_bandwith_max = extract_statistics(spec_bandwith)\n",
    "            flatness_mean, flatness_var, flatness_skew, flatness_kurtosis, flatness_median, flatness_min, flatness_max = extract_statistics(flatness)\n",
    "            rolloff_mean, rolloff_var, rolloff_skew, rolloff_kurtosis, rolloff_median, rolloff_min, rolloff_max = extract_statistics(rolloff)\n",
    "            zero_crossing_rate_mean, zero_crossing_rate_var, zero_crossing_rate_skew, zero_crossing_rate_kurtosis, zero_crossing_rate_median, zero_crossing_rate_min, zero_crossing_rate_max = extract_statistics(zero_crossing_rate)\n",
    "            oenv_mean, oenv_var, oenv_skew, oenv_kurtosis, oenv_median, oenv_min, oenv_max = extract_statistics(oenv)\n",
    "            tempogram_mean, tempogram_var, tempogram_skew, tempogram_kurtosis, tempogram_median, tempogram_min, tempogram_max = extract_statistics(tempogram)\n",
    "            ac_global_mean, ac_global_var, ac_global_skew, ac_global_kurtosis, ac_global_median, ac_global_min, ac_global_max = extract_statistics(ac_global)                                                              \n",
    "                        \n",
    "            #print('Video')\n",
    "            features[i][4] = mfccs_mean\n",
    "            features[i][5] = mfccs_var \n",
    "            features[i][6] = mfccs_skew\n",
    "            features[i][7] = mfccs_kurtosis\n",
    "            features[i][8] = mfccs_median\n",
    "            features[i][9] = mfccs_min\n",
    "            features[i][10] = mfccs_max\n",
    "            #print('MFCC')\n",
    "            features[i][11] = chroma_stft_mean\n",
    "            features[i][12] = chroma_stft_var\n",
    "            features[i][13] = chroma_stft_skew\n",
    "            features[i][14] = chroma_stft_kurtosis\n",
    "            features[i][15] = chroma_stft_median\n",
    "            features[i][16] = chroma_stft_min\n",
    "            features[i][17] = chroma_stft_max\n",
    "            #print('Chroma')\n",
    "            features[i][18] = mel_mean\n",
    "            features[i][19] = mel_var\n",
    "            features[i][20] = mel_skew\n",
    "            features[i][21] = mel_kurtosis\n",
    "            features[i][22] = mel_median\n",
    "            features[i][23] = mel_min\n",
    "            features[i][24] = mel_max\n",
    "            #print('Mel')\n",
    "            features[i][25] = contrast_mean\n",
    "            features[i][26] = contrast_var\n",
    "            features[i][27] = contrast_skew\n",
    "            features[i][28] = contrast_kurtosis\n",
    "            features[i][29] = contrast_median\n",
    "            features[i][30] = contrast_min\n",
    "            features[i][31] = contrast_max\n",
    "            #print('Contrast')\n",
    "            features[i][32] = tonnetz_mean\n",
    "            features[i][33] = tonnetz_var\n",
    "            features[i][34] = tonnetz_skew\n",
    "            features[i][35] = tonnetz_kurtosis\n",
    "            features[i][36] = tonnetz_median\n",
    "            features[i][37] = tonnetz_min\n",
    "            features[i][38] = tonnetz_max\n",
    "            #print('Tonnetz')\n",
    "            features[i][39] = tempo_bpm \n",
    "            #print('Tempo')\n",
    "            features[i][40] = spectral_centroid_mean\n",
    "            features[i][41] = spectral_centroid_var\n",
    "            features[i][42] = spectral_centroid_skew\n",
    "            features[i][43] = spectral_centroid_kurtosis\n",
    "            features[i][44] = spectral_centroid_median\n",
    "            features[i][45] = spectral_centroid_min\n",
    "            features[i][46] = spectral_centroid_max\n",
    "            #print('Spectral Centroid')\n",
    "            features[i][47] = spec_bandwith_mean\n",
    "            features[i][48] = spec_bandwith_var\n",
    "            features[i][49] = spec_bandwith_skew\n",
    "            features[i][50] = spec_bandwith_kurtosis\n",
    "            features[i][51] = spec_bandwith_median\n",
    "            features[i][52] = spec_bandwith_min\n",
    "            features[i][53] = spec_bandwith_max            \n",
    "            #print('Spectral Bandwith')\n",
    "            features[i][54] = flatness_mean\n",
    "            features[i][55] = flatness_var\n",
    "            features[i][56] = flatness_skew\n",
    "            features[i][57] = flatness_kurtosis\n",
    "            features[i][58] = flatness_median\n",
    "            features[i][59] = flatness_min\n",
    "            features[i][60] = flatness_max\n",
    "            #print('Flatness')\n",
    "            features[i][61] = rolloff_mean\n",
    "            features[i][62] = rolloff_var\n",
    "            features[i][63] = rolloff_skew\n",
    "            features[i][64] = rolloff_kurtosis\n",
    "            features[i][65] = rolloff_median\n",
    "            features[i][66] = rolloff_min\n",
    "            features[i][67] = rolloff_max\n",
    "            #print('Rolloff')\n",
    "            features[i][68] = zero_crossing_rate_mean\n",
    "            features[i][69] = zero_crossing_rate_var\n",
    "            features[i][70] = zero_crossing_rate_skew\n",
    "            features[i][71] = zero_crossing_rate_kurtosis\n",
    "            features[i][72] = zero_crossing_rate_median\n",
    "            features[i][73] = zero_crossing_rate_min            \n",
    "            features[i][74] = zero_crossing_rate_max\n",
    "            #print('Zero Crossing Rate')\n",
    "            features[i][75] = oenv_mean\n",
    "            features[i][76] = oenv_var\n",
    "            features[i][77] = oenv_skew\n",
    "            features[i][78] = oenv_kurtosis\n",
    "            features[i][79] = oenv_median\n",
    "            features[i][80] = oenv_min\n",
    "            features[i][81] = oenv_max\n",
    "            #print('Oenv')\n",
    "            features[i][82] = tempogram_mean\n",
    "            features[i][83] = tempogram_var\n",
    "            features[i][84] = tempogram_skew\n",
    "            features[i][85] = tempogram_kurtosis\n",
    "            features[i][86] = tempogram_median\n",
    "            features[i][87] = tempogram_min\n",
    "            features[i][88] = tempogram_max\n",
    "            #print('Tempogram')\n",
    "            features[i][89] = ac_global_mean\n",
    "            features[i][90] = ac_global_var\n",
    "            features[i][91] = ac_global_skew\n",
    "            features[i][92] = ac_global_kurtosis\n",
    "            features[i][93] = ac_global_median\n",
    "            features[i][94] = ac_global_min\n",
    "            features[i][95] = ac_global_max        \n",
    "            #print('Ac Global')\n",
    "    '''\n",
    "    #except Exception as e:\n",
    "    #    print(\"error audio extraction: {0}\".format(e))\n",
    "    #    pass\n",
    "            \n",
    "             \n",
    "\n",
    "def extract_XY(trailer_ids, trailer_labels, folder, num_features = low_level_features, num_genres = 9): #num_features = 96\n",
    "    n_cpus = multiprocessing.cpu_count()    \n",
    "    print('n_cpus:',n_cpus)      \n",
    "    Genres = []    \n",
    "    FullPlots = [] \n",
    "    max_movies = len(trailer_ids) #min(1000,len(trailer_ids))\n",
    "    poster_images = np.zeros((max_movies, width, height, depth))\n",
    "    spectograms = np.zeros((max_movies, audio_channels, sr))\n",
    "    Y = np.zeros((max_movies, num_genres))  #  \n",
    "    features = np.zeros((max_movies, num_features))\n",
    "    lmtd_features = np.zeros((max_movies, time_steps, nb_features))\n",
    "    movie_data = lmtd.get_data_by_trailer_ids(trailer_ids)\n",
    "    i=0\n",
    "    for _id in trailer_ids:  \n",
    "        if i < max_movies:     \n",
    "            print(i)                        \n",
    "            process_movie(movie_data, trailer_labels, _id, i, Y, features, Genres, FullPlots, spectograms, lmtd_features) \n",
    "            img_path = \"/home/paulo/mestrado/posters/\" + folder + \"/\" + _id + \".jpg\"\n",
    "            try:\n",
    "                pil_img = image.load_img(img_path, target_size=(width,height))        \n",
    "                pil_img = pil_img.resize((width,height), Image.ANTIALIAS)\n",
    "                if i == 0:\n",
    "                    plt.imshow(pil_img)\n",
    "                    plt.show()\n",
    "                poster_images[i] = image.img_to_array(pil_img)             \n",
    "            except Exception as e:\n",
    "                print(\"error poster extraction: {0}\".format(e))\n",
    "                pass\n",
    "        i+=1            \n",
    "    X = features\n",
    "    print(X.shape)    \n",
    "    print(X)\n",
    "    print(Y.shape)\n",
    "    print(Y)\n",
    "    print(Genres)\n",
    "    return X, Y, Genres, FullPlots, poster_images, spectograms, lmtd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_trailer_ids = lmtd.train_ids\n",
    "validation_trailer_ids = lmtd.valid_ids\n",
    "test_trailer_ids = lmtd.test_ids\n",
    "\n",
    "'''\n",
    "all_ids = list()\n",
    "all_ids.extend(training_trailer_ids)\n",
    "all_ids.extend(validation_trailer_ids)\n",
    "all_ids.extend(test_trailer_ids)\n",
    "all_ids = set(all_ids)\n",
    "print('Set length movie_trailers:', len(all_ids))\n",
    "\n",
    "all_movie_titles = set()\n",
    "\n",
    "i = 0\n",
    "with tqdm(total=len(all_ids), file=sys.stdout) as pbar:\n",
    "        movie_data = lmtd.get_data_by_trailer_ids(all_ids)\n",
    "        for _id in all_ids:    \n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "            # Returns a dictionary in which keys are the queried trailer_ids            \n",
    "            title_db = movie_data[_id]['Title']\n",
    "            all_movie_titles.add(title_db)\n",
    "        i+=1\n",
    "        \n",
    "print('Diferent movie titles:', len(all_movie_titles))\n",
    "print(all_movie_titles)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, train_Genres, train_FullPlots, train_poster_images, train_spectograms, train_lmtd_features = extract_XY(training_trailer_ids, lmtd.train_labels, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_spectograms.shape)\n",
    "print(train_spectograms[2])\n",
    "print(train_lmtd_features[2])\n",
    "print(train_X[2])\n",
    "print(train_poster_images[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Correlations among Variables and Output Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "name_to_index = {8: 'Action', 7: 'Adventure', 6: 'Comedy', 5: 'Crime', 4: 'Drama', 3: 'Horror', 2: 'Romance', 1: 'SciFi', 0: 'Thriller'}\n",
    "train_Genres_encoded = []\n",
    "for i in range(train_Y.shape[0]):\n",
    "    genres_in_movie = (train_Y[i] == 1)\n",
    "    genre_indexes = [i for i, x in enumerate(genres_in_movie) if x]\n",
    "    #print(genre_indexes)\n",
    "    genres_in_movie_dict = {k: v for k, v in name_to_index.items() if k in genre_indexes and len(genre_indexes)==1}    \n",
    "    genres_in_movie_dict = OrderedDict(sorted(genres_in_movie_dict.items(), key=operator.itemgetter(1)))    \n",
    "    genres_in_movie_str = ','.join(genres_in_movie_dict.values())\n",
    "    print(genres_in_movie_str)\n",
    "    #print(name_to_index[0])\n",
    "    train_Genres_encoded.append(genres_in_movie_str)\n",
    "  \n",
    "d = {'avg_shot_length': train_X[:,0], \n",
    "     'color_var': train_X[:,1], \n",
    "     'light_key': train_X[:,2], \n",
    "     'mot_content': train_X[:,3], \n",
    "     'mfccs_mean': train_X[:,4], \n",
    "     #'chroma_stft_mean': train_X[:,11], \n",
    "     'mel_mean': train_X[:,18], \n",
    "     #'contrast_mean': train_X[:,25], \n",
    "     #'tonnetz_mean': train_X[:,32], \n",
    "     'tempo_bpm': train_X[:,39],\n",
    "     #'spectral_centroid_mean': train_X[:,40],\n",
    "     #'spec_bandwith_mean': train_X[:,47],\n",
    "     'flatness_mean': train_X[:,54],\n",
    "     #'rolloff_mean': train_X[:,61],\n",
    "     'zero_crossing_rate_mean': train_X[:,68],\n",
    "     #'oenv_mean': train_X[:,75],\n",
    "     #'tempogram_mean': train_X[:,82],\n",
    "     #'ac_global_mean': train_X[:,89],\n",
    "     'genres': train_Genres_encoded}\n",
    "print(d)\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df.head())\n",
    "sns.pairplot(df, hue=\"genres\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X, val_Y, val_Genres, val_FullPlots, val_poster_images, val_spectograms, val_lmtd_features = extract_XY(validation_trailer_ids, lmtd.valid_labels, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_Genres, test_FullPlots, test_poster_images, test_spectograms, test_lmtd_features = extract_XY(test_trailer_ids, lmtd.test_labels, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_Genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_X.shape)\n",
    "print(val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_X.shape)\n",
    "print(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = preprocessing.scale(train_X)\n",
    "val_X = preprocessing.scale(val_X)\n",
    "test_X = preprocessing.scale(test_X)\n",
    "train_poster_images = train_poster_images / 255.\n",
    "val_poster_images = val_poster_images / 255.\n",
    "test_poster_images = test_poster_images / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X)\n",
    "print(train_poster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_X)\n",
    "print(val_poster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_X)\n",
    "print(test_poster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_data.extend(train_FullPlots)\n",
    "all_data.extend(val_FullPlots)\n",
    "all_data.extend(test_FullPlots)\n",
    "print(all_data)\n",
    "np.savez('plots', all_data=np.array(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train_text = sequence.pad_sequences(tokenizer.texts_to_sequences(train_FullPlots), maxlen=maxlen)\n",
    "X_val_text = sequence.pad_sequences(tokenizer.texts_to_sequences(val_FullPlots), maxlen=maxlen)\n",
    "X_test_text = sequence.pad_sequences(tokenizer.texts_to_sequences(test_FullPlots), maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape:', X_train_text.shape)\n",
    "print('x_val shape:', X_val_text.shape)\n",
    "print('x_test shape:', X_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(num_classes)\n",
    "\n",
    "y_train = train_Y #to_categorical(train_Genres_encoded, num_classes=num_classes)\n",
    "y_val = val_Y #to_categorical(val_Genres_encoded, num_classes=num_classes)\n",
    "y_test = test_Y #to_categorical(val_Genres_encoded, num_classes=num_classes)\n",
    "#num_classes = y_train.shape[1]\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Matrices (can load afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez('train_matrices', X_train_text=X_train_text, train_poster_images=train_poster_images, train_spectograms=train_spectograms, train_lmtd_features=train_lmtd_features, train_X=train_X, y_train=y_train)\n",
    "#np.savez('val_matrices', X_val_text=X_val_text, val_poster_images=val_poster_images, val_spectograms=val_spectograms, val_lmtd_features=val_lmtd_features, val_X=val_X, y_val=y_val)\n",
    "#np.savez('test_matrices', X_test_text=X_test_text, test_poster_images=test_poster_images, test_spectograms=test_spectograms, test_lmtd_features=test_lmtd_features, test_X=test_X, y_test=y_test)\n",
    "np.savez('train_matrices_text', X_train_text=X_train_text)\n",
    "np.savez('val_matrices_text', X_val_text=X_val_text)\n",
    "np.savez('test_matrices_text', X_test_text=X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save memory\n",
    "del feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftAttention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe(x):\n",
    "    return K.clip(x, K.common._EPSILON, 1.0 - K.common._EPSILON)\n",
    "\n",
    "class Wrapper(Layer):\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        self.layer = layer\n",
    "        super(Wrapper, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):       \n",
    "        super(Wrapper, self).build(input_shape)  # Be sure to call this somewhere!   \n",
    "       \n",
    "    def call(self, x):\n",
    "        super(Wrapper, self).call(x)\n",
    "\n",
    "class ProbabilityTensor(Wrapper):\n",
    "    \"\"\" function for turning 3d tensor to 2d probability matrix, which is the set of a_i's \"\"\"\n",
    "    def __init__(self, dense_function=None, *args, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(ndim=3)]\n",
    "        #layer = TimeDistributed(dense_function) or TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        layer = TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        super(ProbabilityTensor, self).__init__(layer, *args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(shape=input_shape)]\n",
    "        if K._BACKEND == 'tensorflow':\n",
    "            if not input_shape[1]:\n",
    "                raise Exception('When using TensorFlow, you should define '\n",
    "                                'explicitly the number of timesteps of '\n",
    "                                'your sequences.\\n'\n",
    "                                'If your first layer is an Embedding, '\n",
    "                                'make sure to pass it an \"input_length\" '\n",
    "                                'argument. Otherwise, make sure '\n",
    "                                'the first layer has '\n",
    "                                'an \"input_shape\" or \"batch_input_shape\" '\n",
    "                                'argument, including the time axis.')\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ProbabilityTensor, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # b,n,f -> b,n\n",
    "        #       s.t. \\sum_n n = 1\n",
    "        if isinstance(input_shape, (list,tuple)) and not isinstance(input_shape[0], int):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def squash_mask(self, mask):\n",
    "        if K.ndim(mask) == 2:\n",
    "            return mask\n",
    "        elif K.ndim(mask) == 3:\n",
    "            return K.any(mask, axis=-1)\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return self.squash_mask(mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        energy = K.squeeze(self.layer(x), 2)\n",
    "        p_matrix = K.softmax(energy)\n",
    "        if mask is not None:\n",
    "            mask = self.squash_mask(mask)\n",
    "            p_matrix = make_safe(p_matrix * mask)\n",
    "            p_matrix = (p_matrix / K.sum(p_matrix, axis=-1, keepdims=True))*mask\n",
    "        return p_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(ProbabilityTensor, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SoftAttentionConcat(ProbabilityTensor):\n",
    "    '''This will create the context vector and then concatenate it with the last output of the LSTM'''\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # b,n,f -> b,f where f is weighted features summed across n\n",
    "        return (input_shape[0], 2*input_shape[2])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None or mask.ndim==2:\n",
    "            return None\n",
    "        else:\n",
    "            raise Exception(\"Unexpected situation\")\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # b,n,f -> b,f via b,n broadcasted\n",
    "        p_vectors = K.expand_dims(super(SoftAttentionConcat, self).call(x, mask), 2)\n",
    "        expanded_p = K.repeat_elements(p_vectors, K.int_shape(x)[2], axis=2)\n",
    "        context = K.sum(expanded_p * x, axis=1)\n",
    "        last_out = x[:, -1, :]\n",
    "        return K.concatenate([context, last_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = np.load('plots.npz')\n",
    "plots.files\n",
    "all_data=plots['all_data'].tolist() \n",
    "del plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = setup_tokenizer(all_data)\n",
    "word_index = tokenizer.word_index\n",
    "embeddings_index = setup_glove_embeddings()\n",
    "embedding_matrix = setup_embeddings_matrices(embeddings_index, word_index)\n",
    "embedding_layer = setup_embedding_layer(word_index, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    for k, v in result.iteritems():\n",
    "        try:\n",
    "            print '{:<15s}'.format(lmtd.genres[k][2:]),\n",
    "        except IndexError:\n",
    "            print '{:<15s}'.format(str(k)),\n",
    "        print '{:5.4f}'.format(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfiles = np.load('train_matrices.npz', mmap_mode='r')\n",
    "trainfiles_text = np.load('train_matrices_text.npz', mmap_mode='r')\n",
    "trainfiles.files\n",
    "X_train_text=trainfiles_text['X_train_text'] \n",
    "#X_train_text=trainfiles['X_train_text'] \n",
    "train_poster_images=trainfiles['train_poster_images']\n",
    "train_spectograms=trainfiles['train_spectograms']\n",
    "#train_lmtd_features=trainfiles['train_lmtd_features']\n",
    "train_X=trainfiles['train_X']\n",
    "y_train=trainfiles['y_train']\n",
    "del trainfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valfiles = np.load('val_matrices.npz', mmap_mode='r')\n",
    "valfiles_text = np.load('val_matrices_text.npz', mmap_mode='r')\n",
    "valfiles.files\n",
    "X_val_text=valfiles_text['X_val_text']\n",
    "#X_val_text=valfiles['X_val_text']\n",
    "val_poster_images=valfiles['val_poster_images']\n",
    "val_spectograms=valfiles['val_spectograms']\n",
    "#val_lmtd_features=valfiles['val_lmtd_features']\n",
    "val_X=valfiles['val_X']\n",
    "y_val=valfiles['y_val']\n",
    "del valfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfiles = np.load('test_matrices.npz', mmap_mode='r')\n",
    "testfiles_text = np.load('test_matrices_text.npz', mmap_mode='r')\n",
    "testfiles.files\n",
    "X_test_text=testfiles_text['X_test_text']\n",
    "#X_test_text=testfiles['X_test_text']\n",
    "test_poster_images=testfiles['test_poster_images']\n",
    "test_spectograms=testfiles['test_spectograms']\n",
    "#test_lmtd_features=testfiles['test_lmtd_features']\n",
    "test_X=testfiles['test_X'] \n",
    "y_test=testfiles['y_test']\n",
    "del testfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with unbalaced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('class_weight:', {0: 0.1111864406779661, 1: 0.07322033898305084, 2: 0.2047457627118644, 3: 0.08203389830508474, 4: 0.2671186440677966, 5: 0.05288135593220339, 6: 0.08271186440677966, 7: 0.03864406779661017, 8: 0.08745762711864406})\n"
     ]
    }
   ],
   "source": [
    "class_weight = {}\n",
    "weights = np.sum(y_test,axis=0)/np.sum(y_test)\n",
    "for i in range(nb_classes):\n",
    "    class_weight[i] = weights[i]\n",
    "print('class_weight:',class_weight)\n",
    "class_weight = class_weight.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start:', datetime.datetime(2018, 5, 21, 22, 44, 23, 751379))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/librosa/filters.py:271: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paulo/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:88: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/paulo/anaconda2/lib/python2.7/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "audio_spectrogram (InputLayer)  (None, 2, 44100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trainable_stft (Melspectrogram) (None, 64, 345, 2)   74304       audio_spectrogram[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "poster_image (InputLayer)       (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "additive_noise_1 (AdditiveNoise (None, 64, 345, 2)   0           trainable_stft[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 224, 3)  12          poster_image[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normalization2d_1 (Normalizatio (None, 64, 345, 2)   0           additive_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "full_plot_text (InputLayer)     (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 32) 896         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 345, 32)  608         normalization2d_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     2018900     full_plot_text[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 222, 222, 32) 9248        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 62, 343, 32)  9248        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 256)     234496      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 222, 222, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 62, 343, 32)  128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 256)     394240      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 111, 111, 32) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 31, 171, 32)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "soft_attention_concat_1 (SoftAt (None, 512)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 111, 111, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 31, 171, 32)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        soft_attention_concat_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 32)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 32)           0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         525312      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         33792       global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         33792       global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "low_level_features (InputLayer) (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4)            16          low_level_features[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 3076)         0           dropout_1[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2048)         6301696     merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 2048)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "new_features (Dense)            (None, 4096)         8392704     dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 4096)         0           new_features[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 9)            36873       dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,068,441\n",
      "Trainable params: 17,992,971\n",
      "Non-trainable params: 75,470\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paulo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Train on 2874 samples, validate on 374 samples\n",
      "Epoch 1/50\n",
      "2874/2874 [==============================] - 106s 37ms/step - loss: 0.4815 - acc: 0.7852 - val_loss: 0.4429 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44288, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 2/50\n",
      "2874/2874 [==============================] - 89s 31ms/step - loss: 0.4296 - acc: 0.8063 - val_loss: 0.4041 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44288 to 0.40409, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 3/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.3937 - acc: 0.8207 - val_loss: 0.3799 - val_acc: 0.8247\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40409 to 0.37987, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 4/50\n",
      "2874/2874 [==============================] - 89s 31ms/step - loss: 0.3755 - acc: 0.8309 - val_loss: 0.3843 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "2874/2874 [==============================] - 88s 30ms/step - loss: 0.3615 - acc: 0.8341 - val_loss: 0.3664 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.37987 to 0.36639, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 6/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.3491 - acc: 0.8410 - val_loss: 0.3600 - val_acc: 0.8422\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36639 to 0.35996, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 7/50\n",
      "2874/2874 [==============================] - 88s 30ms/step - loss: 0.3366 - acc: 0.8480 - val_loss: 0.3408 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35996 to 0.34082, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 8/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.3268 - acc: 0.8531 - val_loss: 0.3482 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.3163 - acc: 0.8585 - val_loss: 0.3436 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.3051 - acc: 0.8651 - val_loss: 0.3441 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.2991 - acc: 0.8677 - val_loss: 0.3321 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.34082 to 0.33213, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 12/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.2917 - acc: 0.8698 - val_loss: 0.3307 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.33213 to 0.33071, saving model to tensorflow/tmp/weigths_mir_classifier.hdf5\n",
      "Epoch 13/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.2801 - acc: 0.8759 - val_loss: 0.3395 - val_acc: 0.8458\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.2675 - acc: 0.8832 - val_loss: 0.3314 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "2874/2874 [==============================] - 88s 30ms/step - loss: 0.2609 - acc: 0.8862 - val_loss: 0.3489 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "2874/2874 [==============================] - 89s 31ms/step - loss: 0.2487 - acc: 0.8901 - val_loss: 0.3471 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "2874/2874 [==============================] - 88s 31ms/step - loss: 0.2322 - acc: 0.9010 - val_loss: 0.3444 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "773/773 [==============================] - 7s 9ms/step\n",
      "[0.3472134441638517, 0.8431795446869627]\n",
      "Action          0.7006\n",
      "Adventure       0.7461\n",
      "Comedy          0.8268\n",
      "Crime           0.5796\n",
      "Drama           0.7973\n",
      "Horror          0.6655\n",
      "Romance         0.5126\n",
      "SciFi           0.5612\n",
      "Thriller        0.4491\n",
      "macro           0.6488\n",
      "micro           0.7065\n",
      "weighted        0.7009\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print('Start:',start_time)\n",
    "\n",
    "#TIP: set tensorflow number of threads (check number of cores in CPU and use half of maximum)\n",
    "number_gpus = number_gpus\n",
    "number_cpus = number_cpus\n",
    "config = tf.ConfigProto(device_count={'GPU': number_gpus, 'CPU': number_cpus}, log_device_placement=True, intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "use_gpu = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    K.set_session(sess)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    # Branch 1 - RNN for text (Full Plot)\n",
    "    sequence_input = Input(shape=(max_sentence_len,), dtype='int32', name='full_plot_text')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    lstm = Bidirectional(LSTM(lstm_size, return_sequences=True, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n",
    "    lstm2 = Bidirectional(LSTM(lstm_size, return_sequences=True, recurrent_dropout=recurrent_dropout))(lstm)\n",
    "    attention_1 = SoftAttentionConcat()(lstm2)\n",
    "    batch1 = BatchNormalization()(attention_1)\n",
    "    branch_1 = Dense(1024, activation='relu')(batch1)\n",
    "    branch_1 = Dropout(0.5)(branch_1)\n",
    "    \n",
    "    # Branch 2 - CNN for images (Poster)\n",
    "    '''\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    # Freeze Inception's weights - we don't want to train these\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # add a fully connected layer after Inception - we do want to train these\n",
    "    branch2 = base_model.output               \n",
    "    '''\n",
    "    poster_input = Input(shape=(width, height, depth), name='poster_image')   \n",
    "    branch2 = BatchNormalization()(poster_input)\n",
    "    branch2 = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    branch2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    branch2 = BatchNormalization()(branch2)\n",
    "    branch2 = MaxPooling2D(pool_size=(2, 2))(branch2)    \n",
    "    branch2 = Dropout(0.25)(branch2)               \n",
    "    branch2 = GlobalAveragePooling2D()(branch2)\n",
    "    branch2 = Dense(1024, activation='relu')(branch2)\n",
    "    branch2 = Dropout(0.5)(branch2)\n",
    "    \n",
    "    \n",
    "    # Branch 3 - CNN for spectrogram image (Audio)\n",
    "    audio_inputs = Input(shape=audio_input_shape, name='audio_spectrogram')\n",
    "    #A mel-spectrogram layer    \n",
    "    branch3 = Melspectrogram(n_dft=256, n_hop=128,\n",
    "                             padding='same', sr=sr, n_mels=64,\n",
    "                             fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                             return_decibel_melgram=False, trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='trainable_stft')(audio_inputs)\n",
    "    # Maybe some additive white noise.\n",
    "    branch3 = AdditiveNoise(power=0.2)(branch3)\n",
    "    # If you wanna normalise it per-frequency\n",
    "    branch3 = Normalization2D(str_axis='freq')(branch3) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "    #CNN\n",
    "    branch3 = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    branch3 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    branch3 = BatchNormalization()(branch3)\n",
    "    branch3 = MaxPooling2D(pool_size=(2, 2))(branch3)\n",
    "    branch3 = Dropout(0.25)(branch3)   \n",
    "    branch3 = GlobalAveragePooling2D()(branch3)\n",
    "    branch3 = Dense(1024, activation='relu')(branch3)\n",
    "    branch3 = Dropout(0.5)(branch3)\n",
    "    \n",
    "    #Branch 4 - LTMD9 Features                    \n",
    "    lmtd_inputs = Input(shape=(time_steps, nb_features), name='lmtd9_features')    \n",
    "    branch4 = BatchNormalization()(lmtd_inputs) \n",
    "    branch4 = Convolution1D(conv_filters, kernel_size=3)(branch4)\n",
    "    branch4 = BatchNormalization()(branch4)\n",
    "    branch4 = Activation('relu')(branch4)\n",
    "    branch4 = GlobalMaxPooling1D()(branch4)\n",
    "    branch4 = Dropout(dropout)(branch4)\n",
    "    branch4 = Dense(2048, activation='relu')(branch4)\n",
    "    branch4 = Dropout(0.5)(branch4)\n",
    "    \n",
    "    #Branch5 - Low Level Features\n",
    "    low_level_inputs = Input(shape=(low_level_features,), name='low_level_features')\n",
    "    branch5 = BatchNormalization()(low_level_inputs)\n",
    "    \n",
    "    #join branches\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([branch_1, branch2, branch3, branch5], mode='concat')\n",
    "    joint = Dense(2048, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    joint = Dense(4096, activation='relu', name='new_features')(joint)\n",
    "    joint = Dropout(0.5)(joint)    \n",
    "    predictions = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(joint)\n",
    "\n",
    "    full_model = Model(inputs=[sequence_input, poster_input, audio_inputs, low_level_inputs], outputs=[predictions]) \n",
    "    \n",
    "    full_model.summary()\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    #sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='tensorflow/logs', histogram_freq=0, write_graph=False, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='tensorflow/tmp/weigths_mir_classifier.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=0, mode=\"auto\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    full_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = full_model.fit([X_train_text, train_poster_images, train_spectograms,train_X], y_train, \n",
    "                         epochs=max_epochs, batch_size=batch_size, verbose=1, class_weight=class_weight,\n",
    "                         validation_data=([X_val_text, val_poster_images, val_spectograms,val_X ], y_val), #validation_split=0.2, \n",
    "                         shuffle=True, callbacks=[earlystop,reduce_lr, checkpointer,tensorboard])\n",
    "    \n",
    "    full_model.save('MovieGenreClassifier.h5')\n",
    "    \n",
    "    y_pred_test = full_model.predict([X_test_text, test_poster_images, test_spectograms, test_X]) #\n",
    "    print(full_model.evaluate([X_test_text, test_poster_images, test_spectograms, test_X], y_test, verbose=1)) #\n",
    "    result = evaluation.prauc(y_test, y_pred_test)\n",
    "    \n",
    "    print_result(result)\n",
    "    \n",
    "    #Prepare for SVM after\n",
    "    layer_name = 'new_features'\n",
    "    intermediate_layer_model = Model(inputs=[sequence_input, poster_input, audio_inputs, low_level_inputs],\n",
    "                                 outputs=full_model.get_layer(layer_name).output)\n",
    "    intermediate_output_train = intermediate_layer_model.predict([X_train_text, train_poster_images, train_spectograms, train_X])\n",
    "    intermediate_output_val = intermediate_layer_model.predict([X_val_text, val_poster_images, val_spectograms, val_X])\n",
    "    intermediate_output_test = intermediate_layer_model.predict([X_test_text, test_poster_images, test_spectograms, test_X])                                  \n",
    "\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "print('End:',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = np.sum(y_test,axis=0)\n",
    "y_predicted = np.sum(1.*(y_pred_test>0.25), axis=0)\n",
    "print('y target:', y_target)\n",
    "print('y predicted', y_predicted)\n",
    "print(1.*(y_pred_test>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding low level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intermediate_output_train.shape)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high_level_features_train = np.concatenate((intermediate_output_train, train_X), axis=1)\n",
    "low_high_level_features_val = np.concatenate((intermediate_output_val, val_X), axis=1)\n",
    "low_high_level_features_test = np.concatenate((intermediate_output_test, test_X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_high_level_features_train.shape)\n",
    "print(low_high_level_features_val.shape)\n",
    "print(low_high_level_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify\n",
    "class_str = 'SVC'\n",
    "C_best_param = 0.001\n",
    "if 'SVC' in class_str:    \n",
    "    kernel_best_param='rbf' #'linear'    \n",
    "    classifier = SVC(C=C_best_param, kernel=kernel_best_param, random_state=13, class_weight='balanced')\n",
    "elif 'Logistic' in class_str:    \n",
    "    classifier = LogisticRegression(solver='sag', max_iter=100, C=C_best_param, random_state=13, class_weight='balanced', multi_class='ovr', n_jobs=6)\n",
    "\n",
    "ovr = OneVsRestClassifier(classifier, n_jobs=6)\n",
    "ovr.fit(low_high_level_features_train, y_train)\n",
    "test_Ypred = ovr.predict(low_high_level_features_test)\n",
    "print(test_Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Average precision micro,macro, weighted:', evaluation.prauc(y_test,test_Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
